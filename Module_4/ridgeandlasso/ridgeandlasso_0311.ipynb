{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression\n",
    "\n",
    "## SWBAT Implement a regularize regression model to help improve the performance of their model on a test set of data.\n",
    "\n",
    "### Key Questions:\n",
    "\n",
    "* When should you use a regularized model instead of a normal model?\n",
    "\n",
    "* How do regularized models differ from normal linear regression?\n",
    "\n",
    "* How does a Ridge model differ from a Lasso Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Overfiting a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](bias-variance-train-test-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two common (and somewhat related) ways to think of model complexity:\n",
    "\n",
    "1. Model complexity as a function of the total number of features with nonzero weights. \n",
    "\n",
    "*The more features the more complex the model.*\n",
    "\n",
    "2. Model complexity as a function of the weights of all the features in the model. \n",
    "\n",
    "*The bigger the weights of the features (coefficients) the more complex the model.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does model complexity change with coefficients?\n",
    "\n",
    "Recall the way we interpret a regression model $Y = mx + b$: \n",
    "\n",
    "With every unit increase in x, the outcome y increase by m unit. Therefore, the bigger the coefficient m is, the more the outcome is subjected to changes in predictor x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "A cost function is a measure of how good or bad the model is at estimating the relationship of our X and y variables. Usually, it is expressed in the difference between actual values and predicted values. When fitting a model to data, the model finds the coeeficients that will minimize the cost fuction. \n",
    "\n",
    "For simple linear regression, the cost function is represented as:\n",
    "<center> $$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - (mx_i + b))^2$$\n",
    "    \n",
    "    \n",
    "For linear regression with multiple predictors, the cost function is expressed as:\n",
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2$$\n",
    "\n",
    "Where k stands for number of predictors at jth term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Rgression (L2 Norm)\n",
    "\n",
    "The ridge regression applies a penalizing parameter $\\lambda$ *slope* $^2$, such that a small bias will be introduced to the entire model depending on the value of $\\lambda$, which is called a **hyperparameter**. \n",
    "\n",
    "$$ \\text{Ridge Cost Function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p m_j^2$$\n",
    "\n",
    "The ridge regression penalty term contains all of the coefficients squared from the original regression line except for the intercept term. \n",
    "\n",
    "Applying such a penalizing parameter to the cost function, the regression model will now try to minimize the combination of both the residual sum of squares **and** the term $\\lambda \\sum_{j=1}^p m_j^2$. \n",
    "\n",
    "Ridge regression works by reducing the magnitude of the coefficient m and therefore reducing the effect the predictors have on the outcome. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Ridge regression shrinks the coefficients and it helps to reduce the model complexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1 Norm)\n",
    "Lasso regression is very similar to Ridge regression except for one difference - the penalty term is not squared but the absolute values of the coefficients muliplied by lambda, expressed by:\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$\n",
    "\n",
    "The biggest difference in Ridge and Lasso is that Lasso simultaneously performs variable selection: some coefficients are shrunk to 0, rendering them nonexistence in the original regression model. Therefore, Lasso regression performs very well when you have higher dimensional dataset where some predictors are useless; whereas Ridge works best when all the predictors are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "A model **hyperparameter** is a configuration that is external to the model and whose value cannot be estimated from data.\n",
    "\n",
    "\n",
    "- They are often used in processes to help estimate model parameters.\n",
    "- They are often specified by the practitioner.\n",
    "- They can often be set using heuristics.\n",
    "- They are often tuned for a given predictive modeling problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the approriate $\\lambda$\n",
    "\n",
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit:\n",
    "\n",
    "* If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions.\n",
    "\n",
    "* If your lambda value is too low, your model will be more complex, and you run the risk of overfitting your data. Your model will learn too much about the particularities of the training data, and won't be able to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical Use Cases\n",
    "- Ridge: It is majorly used to prevent overfitting. Since it includes all the features, it is not very useful in case of exorbitantly high #features, say in millions, as it will pose computational challenges.\n",
    "- Lasso: Since it provides sparse solutions, it is generally the model of choice (or some variant of this concept) for modelling cases where the #features are in millions or more. In such a case, getting a sparse solution is of great computational advantage as the features with zero coefficients can simply be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence of Highly Correlated Features\n",
    "- Ridge: It generally works well even in presence of highly correlated features as it will include all of them in the model but the coefficients will be distributed among them depending on the correlation.\n",
    "- Lasso: It arbitrarily selects any one feature among the highly correlated ones and reduced the coefficients of the rest to zero. Also, the chosen variable changes randomly with change in model parameters. This generally doesn’t work that well as compared to ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 14})\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_columns', 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import our data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('kc_house_data.csv',index_col='id')\n",
    "df['yr_old']=2017 - df['yr_built']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.price\n",
    "features = df.drop(['price','date','zipcode'],  axis=1 )\n",
    "columns =  features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=34,test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "\n",
    "scalar.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled  = scalar.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train_scaled,y_train)\n",
    "y_train_pred = lm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "print('Root Mean Squared Error:' , train_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lm.predict(X_test_scaled)\n",
    "\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "print('Root Mean Squared Error:' + str(np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#residual plot\n",
    "\n",
    "sns.residplot(y_test_pred, y_test, lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(data=lm.coef_ ).T\n",
    "coef.columns = columns\n",
    "\n",
    "model_coef = coef.T.sort_values(by=0).T\n",
    "model_coef.plot(kind='bar', title='Modal Coefficients', legend=True, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_2.fit(X_train)\n",
    "X_train_2= pd.DataFrame(poly_2.transform(X_train), columns=poly_2.get_feature_names(columns))\n",
    "\n",
    "columns_2  = poly_2.get_feature_names(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_2 = StandardScaler()\n",
    "\n",
    "scalar_2.fit(X_train_2)\n",
    "X_train_2_scaled  = scalar_2.transform(X_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "model2 = lm2.fit(X_train_2_scaled, y_train)\n",
    "y_train_2_pred = lm2.predict(X_train_2_scaled)\n",
    "\n",
    "train_2_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2= pd.DataFrame(poly_2.transform(X_test), columns=poly_2.get_feature_names(columns))\n",
    "X_test_2_scaled = scalar_2.transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred_2 = lm2.predict(X_test_2_scaled)\n",
    "\n",
    "test_2_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred_2))\n",
    "\n",
    "# test2_mae = metrics.mean_absolute_error(y_test2, y_pred2)\n",
    "\n",
    "print(train_2_rmse, test_2_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef2 = pd.DataFrame(data=lm2.coef_ ).T\n",
    "coef2.columns = X_train_2.columns\n",
    "\n",
    "model_coef2 = coef2.T.sort_values(by=0).T\n",
    "model_coef2.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coef2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_test_pred_2, y_test, lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Ridge Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "## training the model\n",
    "\n",
    "ridgeReg = Ridge(alpha=0.1, normalize=True)\n",
    "\n",
    "ridgeReg.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "y_pred_ridge = ridgeReg.predict(X_test_2_scaled)\n",
    "\n",
    "#calculating rmse\n",
    "RMSE_R01 =np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge))\n",
    "print('Test RMSE:', RMSE_R01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))/ y_test.outcome.std())\n",
    "ridge_coef_01 = pd.DataFrame(data=ridgeReg.coef_).T\n",
    "ridge_coef_01.columns = X_test_2.columns\n",
    "ridge_coef_01 = ridge_coef_01.T.sort_values(by=0).T\n",
    "ridge_coef_01.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_pred_ridge, y_test, lowess=True, color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeReg_1 = Ridge(alpha=1, normalize=True)\n",
    "\n",
    "ridgeReg_1.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "y_pred_ridge = ridgeReg_1.predict(X_test_2_scaled)\n",
    "\n",
    "#calculating rmse\n",
    "RMSE_R1 =np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge))\n",
    "print('Test RMSE:', RMSE_R1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ridge_coef_1 = pd.DataFrame(data=ridgeReg_1.coef_).T\n",
    "ridge_coef_1.columns = X_test_2.columns\n",
    "ridge_coef_1 = ridge_coef_1.T.sort_values(by=0).T\n",
    "ridge_coef_1.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef_1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "\n",
    "ridgeReg_5 = Ridge(alpha=5, normalize=True)\n",
    "\n",
    "ridgeReg_5.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "y_pred_ridge = ridgeReg_5.predict(X_test_2_scaled)\n",
    "\n",
    "#calculating rmse\n",
    "RMSE_R5 =np.sqrt(metrics.mean_squared_error(y_test, y_pred_ridge))\n",
    "\n",
    "# print('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('RMSE:', RMSE_R5)\n",
    "# print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))/ y_test.outcome.std())\n",
    "ridge_coef_5 = pd.DataFrame(data=ridgeReg_5.coef_).T\n",
    "ridge_coef_5.columns = X_test_2.columns\n",
    "ridge_coef_5 = ridge_coef_5.T.sort_values(by=0).T\n",
    "ridge_coef_5.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"0 Regularization: \", test_2_rmse, \"\\n\", \n",
    "      \".1 Regularization: \",RMSE_R01, \"\\n\",\n",
    "      \"1 Regularization: \",RMSE_R1, \"\\n\",\n",
    "      \"5 Regularization: \",RMSE_R5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coefs =  pd.concat([ridge_coef_01, ridge_coef_1, ridge_coef_5], sort=True)\n",
    "ridge_coefs.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Points:**\n",
    "* It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n",
    "* It reduces the model complexity by coefficient shrinkage.\n",
    "* It uses L2 regularization technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "lassoReg01 = Lasso(alpha=0.1, normalize=True)\n",
    "\n",
    "lassoReg01.fit(X_train_2_scaled, y_train)\n",
    "\n",
    "y_pred_lasso01 = lassoReg01.predict(X_test_2_scaled)\n",
    "\n",
    "#calculating Rmse\n",
    "RMSE_L01 = np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso01))\n",
    "\n",
    "# print('MSE:', metrics.mean_squared_error(y_test2, y_pred2))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred_lasso01)))\n",
    "# print(np.sqrt(metrics.mean_squared_error(y_test, y_pred))/ y_test.outcome.std())\n",
    "\n",
    "lasso_coef01 = pd.DataFrame(data=lassoReg01.coef_).T\n",
    "lasso_coef01.columns = X_test_2.columns\n",
    "lasso_coef01 = lasso_coef01.T.sort_values(by=0).T\n",
    "lasso_coef01.plot(kind='bar', title='Modal Coefficients', legend=False, figsize=(16,8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice \n",
    "\n",
    "Now  try to rerun these models with different regularization rates to try and figure out which will help to achieve  the lowes  RMSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out a comaprison of the  your Lasso RMSEs\n",
    "\n",
    "print(test_2_rmse, RMSE_L01, RMSE_L1, RMSE_L5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs =  pd.concat([lasso_coef01, lasso_coef1, lasso_coef5], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs.abs().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to end this, let’s summarize what we have learnt so far:\n",
    "\n",
    "1. Cost function of Ridge and Lasso regression and importance of regularization term.\n",
    "2. Went through some examples using simple data-sets to understand Linear regression as a limiting case for both Lasso and Ridge regression.\n",
    "3. Understoodd why Lasso regression can lead to feature selection whereas Ridge can only shrink coefficients close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
