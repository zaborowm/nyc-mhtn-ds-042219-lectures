{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an XGboosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the appropriate packages\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and split data to be used in the models\n",
    "titanic = pd.read_csv('cleaned_titanic.csv', index_col='PassengerId')\n",
    "\n",
    "# Create matrix of features\n",
    "X = titanic.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n",
    "\n",
    "# Create target variable\n",
    "y = titanic['Survived'] # y is the column we're trying to predict\n",
    "\n",
    "# Create a list of the features being used in the \n",
    "feature_cols = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost's hyperparameters\n",
    "\n",
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them [here](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters). But the most common ones that you should know are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- **General Parameters:** Guide the overall functioning\n",
    "- **Booster Parameters:** Guide the individual booster (tree/regression) at each step\n",
    "- **Learning Task Parameters:** Guide the optimization performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- **booster** [default=gbtree]\n",
    "Select the type of model to run at each iteration. It has 2 options:\n",
    "    - gbtree: tree-based models\n",
    "    - gblinear: linear models\n",
    "    \n",
    "- **silent** [default=0]:\n",
    "Silent mode is activated is set to 1, i.e. no running messages will be printed. It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "\n",
    "- **nthread**  [default to maximum number of threads available if not set]\n",
    "This is used for parallel processing and number of cores in the system should be entered. If you wish to run on all cores, value should not be entered and algorithm will detect automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Booster Parameters\n",
    "Though there are 2 types of boosters, we’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "\n",
    "- **eta [default=0.3]**\n",
    "    - Analogous to learning rate in GBM\n",
    "    - Makes the model more robust by shrinking the weights on each step\n",
    "    - Typical final values to be used: 0.01-0.2\n",
    "- **min_child_weight [default=1]**\n",
    "    - Defines the minimum sum of weights of all observations required in a child.\n",
    "    - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- **max_depth [default=6]**\n",
    "    - The maximum depth of a tree, same as GBM.\n",
    "    - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "    - Should be tuned using CV.\n",
    "    - Typical values: 3-10\n",
    "- **max_leaf_nodes**\n",
    "    - The maximum number of terminal nodes or leaves in a tree.\n",
    "    - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "    - If this is defined, GBM will ignore max_depth.\n",
    "- **gamma [default=0]**\n",
    "    - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "    - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- **max_delta_step [default=0]**\n",
    "    - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "    - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "    - This is generally not used but you can explore further if you wish.\n",
    "- **subsample [default=1]**\n",
    "    - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "    - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bytree [default=1]**\n",
    "    - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "    - Typical values: 0.5-1\n",
    "- **colsample_bylevel [default=1]**\n",
    "    - Denotes the subsample ratio of columns for each split, in each level.\n",
    "    - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- **lambda [default=1]**\n",
    "    - L2 regularization term on weights (analogous to Ridge regression)\n",
    "    - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- **alpha [default=0]**\n",
    "    - L1 regularization term on weight (analogous to Lasso regression)\n",
    "    - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- **scale_pos_weight [default=1]**\n",
    "    - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Task Parameters\n",
    "\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "\n",
    "- **objective [default=reg:linear]**\n",
    "    - This defines the loss function to be minimized. Mostly used values are:\n",
    "        - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "        - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "                - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "        - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "- **eval_metric [ default according to objective ]**\n",
    "    - The metric to be used for validation data.\n",
    "    - The default values are rmse for regression and error for classification.\n",
    "    - Typical values are:\n",
    "            - rmse – root mean square error\n",
    "            - mae – mean absolute error\n",
    "            - logloss – negative log-likelihood\n",
    "            - error – Binary classification error rate (0.5 threshold)\n",
    "            - merror – Multiclass classification error rate\n",
    "            - mlogloss – Multiclass logloss\n",
    "            - auc: Area under the curve\n",
    "- **seed [default=0]**\n",
    "    - The random number seed.\n",
    "    - Can be used for generating reproducible results and also for parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning with Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "              max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.3, \n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bytree=0.3, gamma=0, learning_rate=0.1,\n",
       "              max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "              n_estimators=100, n_jobs=1, nthread=None,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "              subsample=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.802691\n",
      "F1: 0.702703\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold Cross Validation using XGBoost\n",
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. XGBoost supports k-fold cross validation via the cv() method. All you have to do is specify the nfolds parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this link) like:\n",
    "\n",
    "- **num_boost_round**: denotes the number of trees you build (analogous to n_estimators)\n",
    "- **metrics:** tells the evaluation metrics to be watched during CV\n",
    "- **as_pandas**: to return the results in a pandas DataFrame.\n",
    "- **early_stopping_rounds: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- **seed**: for reproducibility of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running your model, you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swilson5/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/Users/swilson5/anaconda3/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    }
   ],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\":\"binary:logistic\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 2, \n",
    "          'alpha': 1}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, \n",
    "                    params=params, \n",
    "                    nfold=5,\n",
    "                    num_boost_round=500,\n",
    "                    early_stopping_rounds=5,\n",
    "                    metrics=\"logloss\", \n",
    "                    as_pandas=True, \n",
    "                    seed=123)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-logloss-mean</th>\n",
       "      <th>train-logloss-std</th>\n",
       "      <th>test-logloss-mean</th>\n",
       "      <th>test-logloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.685093</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.671230</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.675102</td>\n",
       "      <td>0.008816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659008</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.663125</td>\n",
       "      <td>0.007590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.644336</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.650664</td>\n",
       "      <td>0.011789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.643942</td>\n",
       "      <td>0.013259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.623169</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.631285</td>\n",
       "      <td>0.012165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.602033</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.610209</td>\n",
       "      <td>0.013601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.591650</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>0.599721</td>\n",
       "      <td>0.021180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.585461</td>\n",
       "      <td>0.019497</td>\n",
       "      <td>0.593338</td>\n",
       "      <td>0.024632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.578619</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.586940</td>\n",
       "      <td>0.022570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570801</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.580262</td>\n",
       "      <td>0.021636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.560062</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.569363</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.556937</td>\n",
       "      <td>0.020256</td>\n",
       "      <td>0.566690</td>\n",
       "      <td>0.023334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.554872</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.564808</td>\n",
       "      <td>0.022910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.546951</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.557979</td>\n",
       "      <td>0.020297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.541852</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.553449</td>\n",
       "      <td>0.020814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.536363</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.548007</td>\n",
       "      <td>0.021016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.529293</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>0.541409</td>\n",
       "      <td>0.021934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.523798</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>0.536885</td>\n",
       "      <td>0.024915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.019159</td>\n",
       "      <td>0.534507</td>\n",
       "      <td>0.025665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.516376</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.513121</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.527367</td>\n",
       "      <td>0.026611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.509301</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.523904</td>\n",
       "      <td>0.027239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.502340</td>\n",
       "      <td>0.024646</td>\n",
       "      <td>0.517755</td>\n",
       "      <td>0.029960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.495447</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.510592</td>\n",
       "      <td>0.026663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.489835</td>\n",
       "      <td>0.016329</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.024653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.488421</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.504394</td>\n",
       "      <td>0.024912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.485823</td>\n",
       "      <td>0.015821</td>\n",
       "      <td>0.501844</td>\n",
       "      <td>0.026256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.480789</td>\n",
       "      <td>0.013510</td>\n",
       "      <td>0.498244</td>\n",
       "      <td>0.023527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.479223</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>0.025181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.401454</td>\n",
       "      <td>0.010871</td>\n",
       "      <td>0.435937</td>\n",
       "      <td>0.035241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.401162</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.435649</td>\n",
       "      <td>0.035531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.400962</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.035789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.400442</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.435524</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.400060</td>\n",
       "      <td>0.010824</td>\n",
       "      <td>0.435140</td>\n",
       "      <td>0.036231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.399748</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>0.435059</td>\n",
       "      <td>0.036187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.399014</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434782</td>\n",
       "      <td>0.036432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.010926</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.036380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.398242</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.036436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.398027</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>0.434199</td>\n",
       "      <td>0.036489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.397627</td>\n",
       "      <td>0.011141</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>0.036524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.397133</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.433835</td>\n",
       "      <td>0.037015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.396799</td>\n",
       "      <td>0.011203</td>\n",
       "      <td>0.433926</td>\n",
       "      <td>0.037013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>0.433716</td>\n",
       "      <td>0.037159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.396403</td>\n",
       "      <td>0.011355</td>\n",
       "      <td>0.433744</td>\n",
       "      <td>0.037349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.396163</td>\n",
       "      <td>0.011408</td>\n",
       "      <td>0.433820</td>\n",
       "      <td>0.037432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.395454</td>\n",
       "      <td>0.011635</td>\n",
       "      <td>0.433707</td>\n",
       "      <td>0.037657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.037710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.394756</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.433355</td>\n",
       "      <td>0.037991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.394392</td>\n",
       "      <td>0.011326</td>\n",
       "      <td>0.432757</td>\n",
       "      <td>0.038672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.394160</td>\n",
       "      <td>0.011433</td>\n",
       "      <td>0.432578</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.393845</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.432501</td>\n",
       "      <td>0.039088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.393636</td>\n",
       "      <td>0.011348</td>\n",
       "      <td>0.432425</td>\n",
       "      <td>0.039194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.393192</td>\n",
       "      <td>0.011366</td>\n",
       "      <td>0.432096</td>\n",
       "      <td>0.039302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.392460</td>\n",
       "      <td>0.011382</td>\n",
       "      <td>0.431824</td>\n",
       "      <td>0.039392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.392240</td>\n",
       "      <td>0.011336</td>\n",
       "      <td>0.431829</td>\n",
       "      <td>0.039511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.011091</td>\n",
       "      <td>0.431388</td>\n",
       "      <td>0.040072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.391675</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.431431</td>\n",
       "      <td>0.040272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.391332</td>\n",
       "      <td>0.011102</td>\n",
       "      <td>0.431478</td>\n",
       "      <td>0.040197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.391117</td>\n",
       "      <td>0.011108</td>\n",
       "      <td>0.431293</td>\n",
       "      <td>0.040253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-logloss-mean  train-logloss-std  test-logloss-mean  \\\n",
       "0              0.685093           0.000379           0.686558   \n",
       "1              0.671230           0.009434           0.675102   \n",
       "2              0.659008           0.010491           0.663125   \n",
       "3              0.644336           0.014084           0.650664   \n",
       "4              0.636066           0.016528           0.643942   \n",
       "5              0.623169           0.012498           0.631285   \n",
       "6              0.602033           0.009843           0.610209   \n",
       "7              0.591650           0.015947           0.599721   \n",
       "8              0.585461           0.019497           0.593338   \n",
       "9              0.578619           0.018259           0.586940   \n",
       "10             0.570801           0.018858           0.580262   \n",
       "11             0.560062           0.022019           0.569363   \n",
       "12             0.556937           0.020256           0.566690   \n",
       "13             0.554872           0.019017           0.564808   \n",
       "14             0.546951           0.015151           0.557979   \n",
       "15             0.541852           0.011557           0.553449   \n",
       "16             0.536363           0.015298           0.548007   \n",
       "17             0.529293           0.017539           0.541409   \n",
       "18             0.523798           0.019147           0.536885   \n",
       "19             0.520973           0.019159           0.534507   \n",
       "20             0.516376           0.021708           0.530500   \n",
       "21             0.513121           0.022248           0.527367   \n",
       "22             0.509301           0.024330           0.523904   \n",
       "23             0.502340           0.024646           0.517755   \n",
       "24             0.495447           0.019774           0.510592   \n",
       "25             0.489835           0.016329           0.505060   \n",
       "26             0.488421           0.016179           0.504394   \n",
       "27             0.485823           0.015821           0.501844   \n",
       "28             0.480789           0.013510           0.498244   \n",
       "29             0.479223           0.012897           0.496588   \n",
       "..                  ...                ...                ...   \n",
       "102            0.401454           0.010871           0.435937   \n",
       "103            0.401162           0.010739           0.435649   \n",
       "104            0.400962           0.010795           0.435636   \n",
       "105            0.400442           0.010828           0.435524   \n",
       "106            0.400060           0.010824           0.435140   \n",
       "107            0.399748           0.010698           0.435059   \n",
       "108            0.399014           0.010926           0.434782   \n",
       "109            0.398543           0.010926           0.434627   \n",
       "110            0.398242           0.010878           0.434306   \n",
       "111            0.398027           0.010912           0.434199   \n",
       "112            0.397627           0.011141           0.433942   \n",
       "113            0.397133           0.011175           0.433835   \n",
       "114            0.396799           0.011203           0.433926   \n",
       "115            0.396528           0.011347           0.433716   \n",
       "116            0.396403           0.011355           0.433744   \n",
       "117            0.396163           0.011408           0.433820   \n",
       "118            0.395454           0.011635           0.433707   \n",
       "119            0.395105           0.011678           0.433470   \n",
       "120            0.394756           0.011652           0.433355   \n",
       "121            0.394392           0.011326           0.432757   \n",
       "122            0.394160           0.011433           0.432578   \n",
       "123            0.393845           0.011346           0.432501   \n",
       "124            0.393636           0.011348           0.432425   \n",
       "125            0.393192           0.011366           0.432096   \n",
       "126            0.392460           0.011382           0.431824   \n",
       "127            0.392240           0.011336           0.431829   \n",
       "128            0.391888           0.011091           0.431388   \n",
       "129            0.391675           0.011100           0.431431   \n",
       "130            0.391332           0.011102           0.431478   \n",
       "131            0.391117           0.011108           0.431293   \n",
       "\n",
       "     test-logloss-std  \n",
       "0            0.000999  \n",
       "1            0.008816  \n",
       "2            0.007590  \n",
       "3            0.011789  \n",
       "4            0.013259  \n",
       "5            0.012165  \n",
       "6            0.013601  \n",
       "7            0.021180  \n",
       "8            0.024632  \n",
       "9            0.022570  \n",
       "10           0.021636  \n",
       "11           0.024691  \n",
       "12           0.023334  \n",
       "13           0.022910  \n",
       "14           0.020297  \n",
       "15           0.020814  \n",
       "16           0.021016  \n",
       "17           0.021934  \n",
       "18           0.024915  \n",
       "19           0.025665  \n",
       "20           0.025853  \n",
       "21           0.026611  \n",
       "22           0.027239  \n",
       "23           0.029960  \n",
       "24           0.026663  \n",
       "25           0.024653  \n",
       "26           0.024912  \n",
       "27           0.026256  \n",
       "28           0.023527  \n",
       "29           0.025181  \n",
       "..                ...  \n",
       "102          0.035241  \n",
       "103          0.035531  \n",
       "104          0.035789  \n",
       "105          0.036278  \n",
       "106          0.036231  \n",
       "107          0.036187  \n",
       "108          0.036432  \n",
       "109          0.036380  \n",
       "110          0.036436  \n",
       "111          0.036489  \n",
       "112          0.036524  \n",
       "113          0.037015  \n",
       "114          0.037013  \n",
       "115          0.037159  \n",
       "116          0.037349  \n",
       "117          0.037432  \n",
       "118          0.037657  \n",
       "119          0.037710  \n",
       "120          0.037991  \n",
       "121          0.038672  \n",
       "122          0.038866  \n",
       "123          0.039088  \n",
       "124          0.039194  \n",
       "125          0.039302  \n",
       "126          0.039392  \n",
       "127          0.039511  \n",
       "128          0.040072  \n",
       "129          0.040272  \n",
       "130          0.040197  \n",
       "131          0.040253  \n",
       "\n",
       "[132 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_clf)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors, target, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "\n",
    "    return alg\n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore())\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.concat([X_train, y_train], axis=1)\n",
    "target = 'Survived'\n",
    "IDcol = 'PassengerId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>youngin</th>\n",
       "      <th>male</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  youngin  male  Q  S  \\\n",
       "PassengerId                                                             \n",
       "740               3  24.0      0      0   7.8958        0     1  0  1   \n",
       "148               3   9.0      2      2  34.3750        1     0  0  1   \n",
       "876               3  15.0      0      0   7.2250        0     0  0  0   \n",
       "641               3  20.0      0      0   7.8542        0     1  0  1   \n",
       "885               3  25.0      0      0   7.0500        0     1  0  1   \n",
       "\n",
       "             Survived  \n",
       "PassengerId            \n",
       "740                 0  \n",
       "148                 0  \n",
       "876                 1  \n",
       "641                 0  \n",
       "885                 0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Accuracy : 0.8829\n",
      "AUC Score (Train): 0.928483\n"
     ]
    }
   ],
   "source": [
    "#Choose all predictors except target & IDcols\n",
    "predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = xgb.XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=3,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.3,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "alg = modelfit(xgb1, train, predictors, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.807175\n",
      "F1: 0.703448\n"
     ]
    }
   ],
   "source": [
    "preds = alg.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining XGBoost with GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup our parameters for XGBoost to test\n",
    "param_test1 = {\n",
    " 'max_depth':range(2,9,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bytree=0.8,\n",
       "                                     gamma=0, learning_rate=0.1,\n",
       "                                     max_delta_step=0, max_depth=5,\n",
       "                                     min_child_weight=1, missing=None,\n",
       "                                     n_estimators=140, n_jobs=1, nthread=4,\n",
       "                                     objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=27, silent=True,\n",
       "                                     subsample=0.8),\n",
       "             iid=False, n_jobs=4,\n",
       "             param_grid={'max_depth': range(2, 9, 2),\n",
       "                         'min_child_weight': range(1, 6, 2)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate the Gridsearch model\n",
    "gsearch1 = GridSearchCV(\n",
    "    estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27),\n",
    "    param_grid = param_test1, \n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    iid=False, \n",
    "    cv=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.fit(train[predictors],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'min_child_weight': 5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8838145113243383"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.780269\n",
      "F1: 0.662069\n"
     ]
    }
   ],
   "source": [
    "preds = gsearch1.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "test_f1 = f1_score(y_test, preds)\n",
    "test_acc = accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1cb8d9d233b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# plot feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# plot feature importance\n",
    "plot_importance(alg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fare': 371,\n",
       " 'Age': 361,\n",
       " 'Q': 13,\n",
       " 'SibSp': 80,\n",
       " 'male': 52,\n",
       " 'Pclass': 58,\n",
       " 'Parch': 33,\n",
       " 'S': 34,\n",
       " 'youngin': 10}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg.get_booster().get_fscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle list object\n",
    " \n",
    "model_pickle_path = 'xg_boost_model.pkl'\n",
    "\n",
    "# Create an variable to pickle and open it in write mode\n",
    "model_pickle = open(model_pickle_path, 'wb')\n",
    "pickle.dump(gsearch1.best_estimator_, model_pickle)\n",
    "model_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded XGboost model ::  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bytree=0.8, gamma=0, learning_rate=0.1,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=5, missing=nan,\n",
      "              n_estimators=140, n_jobs=1, nthread=4,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
      "              subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Loading the saved XGboost model pickle\n",
    "xgboost_model_pkl = open(model_pickle_path, 'rb')\n",
    "xgboost_model = pickle.load(xgboost_model_pkl)\n",
    "print(\"Loaded XGboost model :: \", xgboost_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(9,12)\n",
    "    for min_child_weight in range(5,8)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 5), (9, 6), (9, 7), (10, 5), (10, 6), (10, 7), (11, 5), (11, 6), (11, 7)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
