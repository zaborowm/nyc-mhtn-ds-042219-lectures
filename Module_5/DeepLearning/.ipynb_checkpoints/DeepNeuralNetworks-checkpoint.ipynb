{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "How does sklearn utilize numpy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Playground](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Number of Hidden Layers**\n",
    "\n",
    "*For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. Very complex tasks, like image classification, will need dozens of layers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Neurons per layer**\n",
    "\n",
    "*The number of nuerons for the input and output layers are dependent on your data and the task. For hiddne layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.*\n",
    "\n",
    "*In general, you will get more bang for your buck by adding on more layers than adding more neurons.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Activation Functions](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)**\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "*In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. For the output layer, the softmax activation function is generally good for multiclass problems and the sigmouid function for binary classificatin problems. For regression tasks, you can simply use no activation function at all*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Selecting an optimizer](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)\n",
    "    - Adam\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate**\n",
    "\n",
    "*If you set it too low, training will eventually converge, but it will do so slowly.*\n",
    "*If you set it too high, it might acutally diverge.*\n",
    "*If you set it slightly too high, it will converge at first but miss the local optima.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Regularization** \n",
    "    - L1 and L2\n",
    "    - Dropout\n",
    "    \n",
    "    *the most popular techniqure for deep neural networks. It is a fairly simple algorithm where at every training step, every neuron has a probability fo being teporarily \"droppedout,\" meaning it will be completely ignored dureing this traing step, but it may be active during the next step.*\n",
    "    \n",
    "    - [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "    \n",
    "    *Just intterupt training whne its performance on the validation set starts dropping*\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper on selecting hyperparameters](https://arxiv.org/pdf/1206.5533v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",header=None, delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>143</td>\n",
       "      <td>94</td>\n",
       "      <td>33</td>\n",
       "      <td>146</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>115</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>160</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>58</td>\n",
       "      <td>18</td>\n",
       "      <td>116</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>140</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>33</td>\n",
       "      <td>105</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6</td>\n",
       "      <td>162</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>74</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>510</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8</td>\n",
       "      <td>154</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>110</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7</td>\n",
       "      <td>137</td>\n",
       "      <td>90</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9</td>\n",
       "      <td>170</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9</td>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1   2   3    4     5      6   7  8\n",
       "0     6  148  72  35    0  33.6  0.627  50  1\n",
       "1     1   85  66  29    0  26.6  0.351  31  0\n",
       "2     8  183  64   0    0  23.3  0.672  32  1\n",
       "3     1   89  66  23   94  28.1  0.167  21  0\n",
       "4     0  137  40  35  168  43.1  2.288  33  1\n",
       "5     5  116  74   0    0  25.6  0.201  30  0\n",
       "6     3   78  50  32   88  31.0  0.248  26  1\n",
       "7    10  115   0   0    0  35.3  0.134  29  0\n",
       "8     2  197  70  45  543  30.5  0.158  53  1\n",
       "9     8  125  96   0    0   0.0  0.232  54  1\n",
       "10    4  110  92   0    0  37.6  0.191  30  0\n",
       "11   10  168  74   0    0  38.0  0.537  34  1\n",
       "12   10  139  80   0    0  27.1  1.441  57  0\n",
       "13    1  189  60  23  846  30.1  0.398  59  1\n",
       "14    5  166  72  19  175  25.8  0.587  51  1\n",
       "15    7  100   0   0    0  30.0  0.484  32  1\n",
       "16    0  118  84  47  230  45.8  0.551  31  1\n",
       "17    7  107  74   0    0  29.6  0.254  31  1\n",
       "18    1  103  30  38   83  43.3  0.183  33  0\n",
       "19    1  115  70  30   96  34.6  0.529  32  1\n",
       "20    3  126  88  41  235  39.3  0.704  27  0\n",
       "21    8   99  84   0    0  35.4  0.388  50  0\n",
       "22    7  196  90   0    0  39.8  0.451  41  1\n",
       "23    9  119  80  35    0  29.0  0.263  29  1\n",
       "24   11  143  94  33  146  36.6  0.254  51  1\n",
       "25   10  125  70  26  115  31.1  0.205  41  1\n",
       "26    7  147  76   0    0  39.4  0.257  43  1\n",
       "27    1   97  66  15  140  23.2  0.487  22  0\n",
       "28   13  145  82  19  110  22.2  0.245  57  0\n",
       "29    5  117  92   0    0  34.1  0.337  38  0\n",
       "..   ..  ...  ..  ..  ...   ...    ...  .. ..\n",
       "738   2   99  60  17  160  36.6  0.453  21  0\n",
       "739   1  102  74   0    0  39.5  0.293  42  1\n",
       "740  11  120  80  37  150  42.3  0.785  48  1\n",
       "741   3  102  44  20   94  30.8  0.400  26  0\n",
       "742   1  109  58  18  116  28.5  0.219  22  0\n",
       "743   9  140  94   0    0  32.7  0.734  45  1\n",
       "744  13  153  88  37  140  40.6  1.174  39  0\n",
       "745  12  100  84  33  105  30.0  0.488  46  0\n",
       "746   1  147  94  41    0  49.3  0.358  27  1\n",
       "747   1   81  74  41   57  46.3  1.096  32  0\n",
       "748   3  187  70  22  200  36.4  0.408  36  1\n",
       "749   6  162  62   0    0  24.3  0.178  50  1\n",
       "750   4  136  70   0    0  31.2  1.182  22  1\n",
       "751   1  121  78  39   74  39.0  0.261  28  0\n",
       "752   3  108  62  24    0  26.0  0.223  25  0\n",
       "753   0  181  88  44  510  43.3  0.222  26  1\n",
       "754   8  154  78  32    0  32.4  0.443  45  1\n",
       "755   1  128  88  39  110  36.5  1.057  37  1\n",
       "756   7  137  90  41    0  32.0  0.391  39  0\n",
       "757   0  123  72   0    0  36.3  0.258  52  1\n",
       "758   1  106  76   0    0  37.5  0.197  26  0\n",
       "759   6  190  92   0    0  35.5  0.278  66  1\n",
       "760   2   88  58  26   16  28.4  0.766  22  0\n",
       "761   9  170  74  31    0  44.0  0.403  43  1\n",
       "762   9   89  62   0    0  22.5  0.142  33  0\n",
       "763  10  101  76  48  180  32.9  0.171  63  0\n",
       "764   2  122  70  27    0  36.8  0.340  27  0\n",
       "765   5  121  72  23  112  26.2  0.245  30  0\n",
       "766   1  126  60   0    0  30.1  0.349  47  1\n",
       "767   1   93  70  31    0  30.4  0.315  23  0\n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the data \n",
    "X = dataset.iloc[:,0:8]\n",
    "y = dataset.iloc[:,8]\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/swilson5/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/swilson5/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#adding in regularization via Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "model.add(Dense(units=16, kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "#Final Layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "network.add(Dropout(0.2, input_shape=(8,)))\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(Dense(units=16, activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "network.add(Dropout(0.25))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "network.add(Dense(units=16, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "#Final Layer\n",
    "network.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using GridSearchCV to tune Neural Networks](https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"460pt\" viewBox=\"0.00 0.00 269.00 460.00\" width=\"269pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 456)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-456 265,-456 265,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 112787061504 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>112787061504</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-332.5 3.5,-378.5 257.5,-378.5 257.5,-332.5 3.5,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.5\" y=\"-351.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-332.5 131.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-355.5 187.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-332.5 187.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-363.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-355.5 257.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-340.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 112787061840 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>112787061840</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-249.5 11.5,-295.5 249.5,-295.5 249.5,-249.5 11.5,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-268.8\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-249.5 116.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-272.5 172.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-249.5 172.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-280.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-272.5 249.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-257.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 112787061504&#45;&gt;112787061840 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>112787061504-&gt;112787061840</title>\n",
       "<path d=\"M130.5,-332.3799C130.5,-324.1745 130.5,-314.7679 130.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-305.784 130.5,-295.784 127.0001,-305.784 134.0001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112787166992 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112787166992</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 261,-212.5 261,-166.5 0,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-185.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"128,-166.5 128,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"128,-189.5 184,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"184,-166.5 184,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-197.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"184,-189.5 261,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-174.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 112787061840&#45;&gt;112787166992 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112787061840-&gt;112787166992</title>\n",
       "<path d=\"M130.5,-249.3799C130.5,-241.1745 130.5,-231.7679 130.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-222.784 130.5,-212.784 127.0001,-222.784 134.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112787167048 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>112787167048</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-83.5 11.5,-129.5 249.5,-129.5 249.5,-83.5 11.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-102.8\">dense_5: Dense</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-83.5 116.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-106.5 172.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-83.5 172.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-114.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-106.5 249.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-91.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 112787166992&#45;&gt;112787167048 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>112787166992-&gt;112787167048</title>\n",
       "<path d=\"M130.5,-166.3799C130.5,-158.1745 130.5,-148.7679 130.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-139.784 130.5,-129.784 127.0001,-139.784 134.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112787165704 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>112787165704</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-.5 11.5,-46.5 249.5,-46.5 249.5,-.5 11.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-19.8\">dense_6: Dense</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-.5 116.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"116.5,-23.5 172.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"144.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-.5 172.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-31.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"172.5,-23.5 249.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 112787167048&#45;&gt;112787165704 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>112787167048-&gt;112787165704</title>\n",
       "<path d=\"M130.5,-83.3799C130.5,-75.1745 130.5,-65.7679 130.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-56.784 130.5,-46.784 127.0001,-56.784 134.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112787062176 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>112787062176</title>\n",
       "<polygon fill=\"none\" points=\"80,-415.5 80,-451.5 181,-451.5 181,-415.5 80,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130.5\" y=\"-429.8\">112787062176</text>\n",
       "</g>\n",
       "<!-- 112787062176&#45;&gt;112787061504 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>112787062176-&gt;112787061504</title>\n",
       "<path d=\"M130.5,-415.4092C130.5,-407.4308 130.5,-397.795 130.5,-388.606\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-388.5333 130.5,-378.5333 127.0001,-388.5334 134.0001,-388.5333\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "#Visualize network architecture\n",
    "SVG(model_to_dot(network, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization as a file\n",
    "plot_model(network, show_shapes=True, to_file='network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Implementation of optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "[Impact of Learning Rate on MOdel Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/swilson5/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/15\n",
      " - 1s - loss: 4.7224 - acc: 0.5895 - val_loss: 3.8663 - val_acc: 0.6220\n",
      "Epoch 2/15\n",
      " - 0s - loss: 4.4592 - acc: 0.6051 - val_loss: 3.6449 - val_acc: 0.6260\n",
      "Epoch 3/15\n",
      " - 0s - loss: 4.5068 - acc: 0.5973 - val_loss: 3.4782 - val_acc: 0.6220\n",
      "Epoch 4/15\n",
      " - 0s - loss: 3.8598 - acc: 0.6070 - val_loss: 3.2596 - val_acc: 0.6142\n",
      "Epoch 5/15\n",
      " - 0s - loss: 3.6883 - acc: 0.6089 - val_loss: 3.0372 - val_acc: 0.5984\n",
      "Epoch 6/15\n",
      " - 0s - loss: 3.4066 - acc: 0.6070 - val_loss: 2.8268 - val_acc: 0.5906\n",
      "Epoch 7/15\n",
      " - 0s - loss: 3.1735 - acc: 0.5875 - val_loss: 2.5867 - val_acc: 0.5709\n",
      "Epoch 8/15\n",
      " - 0s - loss: 3.0515 - acc: 0.5642 - val_loss: 2.2557 - val_acc: 0.5433\n",
      "Epoch 9/15\n",
      " - 0s - loss: 3.0143 - acc: 0.5214 - val_loss: 1.9671 - val_acc: 0.5551\n",
      "Epoch 10/15\n",
      " - 0s - loss: 2.6679 - acc: 0.5428 - val_loss: 1.7814 - val_acc: 0.5236\n",
      "Epoch 11/15\n",
      " - 0s - loss: 2.3595 - acc: 0.5623 - val_loss: 1.6823 - val_acc: 0.5079\n",
      "Epoch 12/15\n",
      " - 0s - loss: 2.4122 - acc: 0.5584 - val_loss: 1.5867 - val_acc: 0.4961\n",
      "Epoch 13/15\n",
      " - 0s - loss: 2.0270 - acc: 0.5486 - val_loss: 1.4712 - val_acc: 0.4882\n",
      "Epoch 14/15\n",
      " - 0s - loss: 2.1232 - acc: 0.5798 - val_loss: 1.3562 - val_acc: 0.4921\n",
      "Epoch 15/15\n",
      " - 0s - loss: 1.9627 - acc: 0.5681 - val_loss: 1.2594 - val_acc: 0.5079\n"
     ]
    }
   ],
   "source": [
    "# Train neural network\n",
    "history = network.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=15, # Number of epochs\n",
    "                      verbose=2, # Some output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 0s 484us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 53.54%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_loss_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history.history['acc']\n",
    "test_accuracy = history.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_performance_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "    \n",
    "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
