{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Off\n",
    "\n",
    "What hyperparameters do we need to tuning when training neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install keras with tensorflow backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. https://keras.io/#installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Model with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Create first network with Keras\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\",header=None, delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the data \n",
    "X = dataset.iloc[:,0:8]\n",
    "y = dataset.iloc[:,8]\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Number of Neurons per layer**\n",
    "\n",
    "The number of nuerons for the input and output layers are dependent on your data and the task. For hiddne layers, a common practice is to create a funnel with funnel with fewer and fewer neurons per layer.\n",
    "\n",
    "In general, you will get more bang for your buck by adding on more layers than adding more neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#1-layer network\n",
    "model = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#Final Layer\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 1ms/step - loss: 6.6951 - acc: 0.4805 - val_loss: 6.4795 - val_acc: 0.4882\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 6.0430 - acc: 0.5253 - val_loss: 5.8864 - val_acc: 0.5039\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 48us/step - loss: 5.7702 - acc: 0.5253 - val_loss: 5.6180 - val_acc: 0.5512\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 5.5135 - acc: 0.5545 - val_loss: 5.4127 - val_acc: 0.5551\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 5.2061 - acc: 0.5584 - val_loss: 5.1993 - val_acc: 0.5748\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 4.8539 - acc: 0.5700 - val_loss: 4.9934 - val_acc: 0.5748\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 4.5306 - acc: 0.5798 - val_loss: 4.7429 - val_acc: 0.5945\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 4.2555 - acc: 0.5856 - val_loss: 4.5293 - val_acc: 0.5945\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 4.0229 - acc: 0.5914 - val_loss: 4.3268 - val_acc: 0.5945\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 3.8664 - acc: 0.6148 - val_loss: 4.1343 - val_acc: 0.5866\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 3.7025 - acc: 0.6187 - val_loss: 3.8313 - val_acc: 0.6024\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 3.5231 - acc: 0.6187 - val_loss: 3.6536 - val_acc: 0.5906\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 3.4465 - acc: 0.6245 - val_loss: 3.5531 - val_acc: 0.6142\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 3.4086 - acc: 0.6206 - val_loss: 3.5219 - val_acc: 0.6102\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 169us/step - loss: 3.3631 - acc: 0.6245 - val_loss: 3.4816 - val_acc: 0.6102\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 3.3282 - acc: 0.6284 - val_loss: 3.4474 - val_acc: 0.6063\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 3.3001 - acc: 0.6089 - val_loss: 3.4100 - val_acc: 0.5984\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 3.2317 - acc: 0.6265 - val_loss: 3.3433 - val_acc: 0.6102\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 3.2465 - acc: 0.6342 - val_loss: 3.3024 - val_acc: 0.6024\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 3.1380 - acc: 0.6304 - val_loss: 3.2269 - val_acc: 0.6102\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 3.0981 - acc: 0.6265 - val_loss: 3.1790 - val_acc: 0.6142\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 3.0341 - acc: 0.6187 - val_loss: 3.0866 - val_acc: 0.6142\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 2.9799 - acc: 0.6323 - val_loss: 2.9952 - val_acc: 0.6024\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 2.8781 - acc: 0.6167 - val_loss: 2.8922 - val_acc: 0.5906\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 2.7601 - acc: 0.6148 - val_loss: 2.7757 - val_acc: 0.5866\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 2.6516 - acc: 0.6070 - val_loss: 2.6515 - val_acc: 0.5827\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 2.4941 - acc: 0.6167 - val_loss: 2.5233 - val_acc: 0.5748\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 2.3226 - acc: 0.6128 - val_loss: 2.3927 - val_acc: 0.5551\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 2.2098 - acc: 0.6206 - val_loss: 2.2668 - val_acc: 0.5827\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 2.1060 - acc: 0.6206 - val_loss: 2.1810 - val_acc: 0.5394\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 1.9938 - acc: 0.6012 - val_loss: 2.0849 - val_acc: 0.5512\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 1.9123 - acc: 0.6187 - val_loss: 2.0114 - val_acc: 0.5276\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 1.8259 - acc: 0.5700 - val_loss: 1.8945 - val_acc: 0.5551\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 1.7803 - acc: 0.6226 - val_loss: 1.8219 - val_acc: 0.5276\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 1.6794 - acc: 0.5428 - val_loss: 1.7221 - val_acc: 0.5591\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 1.5654 - acc: 0.6226 - val_loss: 1.6196 - val_acc: 0.5433\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 1.4669 - acc: 0.5895 - val_loss: 1.5476 - val_acc: 0.5630\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 1.3857 - acc: 0.5992 - val_loss: 1.4943 - val_acc: 0.5276\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 1.3111 - acc: 0.5739 - val_loss: 1.3785 - val_acc: 0.5748\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 1.2364 - acc: 0.6304 - val_loss: 1.3094 - val_acc: 0.5433\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 1.1504 - acc: 0.6012 - val_loss: 1.2438 - val_acc: 0.5394\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 1.0853 - acc: 0.6226 - val_loss: 1.1601 - val_acc: 0.5748\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 32us/step - loss: 1.0259 - acc: 0.6012 - val_loss: 1.1043 - val_acc: 0.5984\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.9888 - acc: 0.6089 - val_loss: 1.0526 - val_acc: 0.5709\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.9198 - acc: 0.6284 - val_loss: 1.0016 - val_acc: 0.6102\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.8812 - acc: 0.6323 - val_loss: 0.9678 - val_acc: 0.5827\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.8720 - acc: 0.5953 - val_loss: 0.9403 - val_acc: 0.6142\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.8332 - acc: 0.6265 - val_loss: 0.9096 - val_acc: 0.5906\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.8148 - acc: 0.6284 - val_loss: 0.8708 - val_acc: 0.5906\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.7979 - acc: 0.6265 - val_loss: 0.8477 - val_acc: 0.6024\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "history = model.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the number of neurons per layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Number of Hidden Layers**\n",
    "\n",
    "For many problems you can start with just one or two hidden layers it will work just fine. For more complex problems, you can gradually ramp up the number of hidden layers until your model starts to over fit. \n",
    "\n",
    "Very complex tasks, like image classification, will need dozens of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer network\n",
    "model2 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model2.add(Dense(12, input_dim=8, activation='relu'))\n",
    "#create second hidden layer\n",
    "model2.add(Dense(8,activation='relu'))\n",
    "#Final Layer\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 2ms/step - loss: 8.8820 - acc: 0.3619 - val_loss: 7.4004 - val_acc: 0.3504\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 5.7677 - acc: 0.3560 - val_loss: 5.1705 - val_acc: 0.3780\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 4.6781 - acc: 0.4377 - val_loss: 4.6342 - val_acc: 0.4882\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 4.3746 - acc: 0.4805 - val_loss: 4.2991 - val_acc: 0.4528\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 3.7635 - acc: 0.4416 - val_loss: 4.0005 - val_acc: 0.4134\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 3.4756 - acc: 0.3716 - val_loss: 3.6663 - val_acc: 0.4134\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 3.2246 - acc: 0.4086 - val_loss: 3.3811 - val_acc: 0.4331\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 2.9640 - acc: 0.4163 - val_loss: 3.0941 - val_acc: 0.4173\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 2.6726 - acc: 0.4008 - val_loss: 2.8078 - val_acc: 0.3858\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 2.3497 - acc: 0.3891 - val_loss: 2.5076 - val_acc: 0.4291\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 49us/step - loss: 2.1554 - acc: 0.4747 - val_loss: 2.3626 - val_acc: 0.3780\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 1.9531 - acc: 0.4241 - val_loss: 2.1804 - val_acc: 0.4724\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 1.7955 - acc: 0.5311 - val_loss: 2.1710 - val_acc: 0.4094\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 48us/step - loss: 1.6762 - acc: 0.4611 - val_loss: 1.9645 - val_acc: 0.4803\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 52us/step - loss: 1.5577 - acc: 0.5195 - val_loss: 1.9282 - val_acc: 0.4724\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 51us/step - loss: 1.4633 - acc: 0.5097 - val_loss: 1.7879 - val_acc: 0.5079\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 54us/step - loss: 1.3861 - acc: 0.5389 - val_loss: 1.7367 - val_acc: 0.4843\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 53us/step - loss: 1.2978 - acc: 0.5272 - val_loss: 1.5977 - val_acc: 0.5157\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 1.2319 - acc: 0.5642 - val_loss: 1.5267 - val_acc: 0.5039\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 1.1586 - acc: 0.5642 - val_loss: 1.4457 - val_acc: 0.5118\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 49us/step - loss: 1.1053 - acc: 0.5700 - val_loss: 1.3531 - val_acc: 0.5630\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 1.0484 - acc: 0.5934 - val_loss: 1.2780 - val_acc: 0.5354\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.9812 - acc: 0.5914 - val_loss: 1.1725 - val_acc: 0.5591\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.9281 - acc: 0.6070 - val_loss: 1.1018 - val_acc: 0.5827\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.8890 - acc: 0.6148 - val_loss: 1.0407 - val_acc: 0.5906\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 51us/step - loss: 0.8576 - acc: 0.6284 - val_loss: 1.0063 - val_acc: 0.6024\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.8689 - acc: 0.6459 - val_loss: 1.0236 - val_acc: 0.5945\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.8084 - acc: 0.6518 - val_loss: 0.9420 - val_acc: 0.5984\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.7745 - acc: 0.6420 - val_loss: 0.9284 - val_acc: 0.5984\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.7578 - acc: 0.6479 - val_loss: 0.9065 - val_acc: 0.6102\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.7489 - acc: 0.6595 - val_loss: 0.9160 - val_acc: 0.5984\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.7347 - acc: 0.6537 - val_loss: 0.8767 - val_acc: 0.6181\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.7247 - acc: 0.6518 - val_loss: 0.8606 - val_acc: 0.6181\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.7101 - acc: 0.6576 - val_loss: 0.8619 - val_acc: 0.6299\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.7065 - acc: 0.6576 - val_loss: 0.8614 - val_acc: 0.5984\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.7044 - acc: 0.6556 - val_loss: 0.8428 - val_acc: 0.6339\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.6884 - acc: 0.6615 - val_loss: 0.8299 - val_acc: 0.6378\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6848 - acc: 0.6576 - val_loss: 0.8233 - val_acc: 0.6378\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.6880 - acc: 0.6634 - val_loss: 0.9304 - val_acc: 0.5591\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.7073 - acc: 0.6304 - val_loss: 0.8154 - val_acc: 0.6614\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6765 - acc: 0.6459 - val_loss: 0.8072 - val_acc: 0.6535\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.6677 - acc: 0.6556 - val_loss: 0.8565 - val_acc: 0.5984\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6654 - acc: 0.6946 - val_loss: 0.8004 - val_acc: 0.6496\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.6567 - acc: 0.6634 - val_loss: 0.8076 - val_acc: 0.6339\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.6489 - acc: 0.6595 - val_loss: 0.8101 - val_acc: 0.6929\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.6831 - acc: 0.6770 - val_loss: 0.8067 - val_acc: 0.6181\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.6688 - acc: 0.6848 - val_loss: 0.8334 - val_acc: 0.6339\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.6454 - acc: 0.6790 - val_loss: 0.8273 - val_acc: 0.6772\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6800 - acc: 0.6576 - val_loss: 0.8008 - val_acc: 0.6339\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.6360 - acc: 0.6887 - val_loss: 0.7735 - val_acc: 0.6614\n"
     ]
    }
   ],
   "source": [
    "#train 2nd model\n",
    "history2 = model2.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model2.predict(X_test)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the number of layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network with different activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Activation Functions](https://towardsdatascience.com/exploring-activation-functions-for-neural-networks-73498da59b02)**\n",
    "    - Linear\n",
    "    - Sigmoid\n",
    "    - Softmax\n",
    "    - Tanh\n",
    "    - ReLu\n",
    "    - elu\n",
    "    \n",
    "In most cases you can use the ReLu activation function (or one of its variants) in the hidden layers. \n",
    "\n",
    "For the output layer, the softmax activation function is generally good for multiclass problems and the sigmouid function for binary classificatin problems. For regression tasks, you can simply use no activation function at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 3ms/step - loss: 0.7007 - acc: 0.6206 - val_loss: 0.6834 - val_acc: 0.6457\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 204us/step - loss: 0.6631 - acc: 0.6459 - val_loss: 0.6695 - val_acc: 0.6457\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 0.6529 - acc: 0.6634 - val_loss: 0.6647 - val_acc: 0.6417\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 200us/step - loss: 0.6451 - acc: 0.6673 - val_loss: 0.6590 - val_acc: 0.6693\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 196us/step - loss: 0.6304 - acc: 0.6634 - val_loss: 0.6631 - val_acc: 0.6850\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 0.6295 - acc: 0.6634 - val_loss: 0.6512 - val_acc: 0.6850\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.6266 - acc: 0.6654 - val_loss: 0.6586 - val_acc: 0.7087\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.6298 - acc: 0.6673 - val_loss: 0.6919 - val_acc: 0.6457\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 186us/step - loss: 0.6423 - acc: 0.6498 - val_loss: 0.6416 - val_acc: 0.6890\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.5980 - acc: 0.6790 - val_loss: 0.6420 - val_acc: 0.6969\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.6082 - acc: 0.6634 - val_loss: 0.6321 - val_acc: 0.6890\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.5977 - acc: 0.6829 - val_loss: 0.6598 - val_acc: 0.6969\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 182us/step - loss: 0.6409 - acc: 0.6751 - val_loss: 0.6262 - val_acc: 0.7165\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 185us/step - loss: 0.6065 - acc: 0.6751 - val_loss: 0.6290 - val_acc: 0.7047\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5933 - acc: 0.6868 - val_loss: 0.6994 - val_acc: 0.6142\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 197us/step - loss: 0.6022 - acc: 0.6829 - val_loss: 0.6295 - val_acc: 0.6929\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 203us/step - loss: 0.5860 - acc: 0.6887 - val_loss: 0.6417 - val_acc: 0.6850\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5862 - acc: 0.6907 - val_loss: 0.6297 - val_acc: 0.6969\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5804 - acc: 0.6926 - val_loss: 0.6362 - val_acc: 0.7087\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 186us/step - loss: 0.5787 - acc: 0.6829 - val_loss: 0.6260 - val_acc: 0.6850\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 176us/step - loss: 0.5804 - acc: 0.6965 - val_loss: 0.6251 - val_acc: 0.6929\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5724 - acc: 0.6907 - val_loss: 0.6215 - val_acc: 0.7008\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5828 - acc: 0.6887 - val_loss: 0.6371 - val_acc: 0.6811\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5716 - acc: 0.6946 - val_loss: 0.6399 - val_acc: 0.6772\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 196us/step - loss: 0.5758 - acc: 0.6926 - val_loss: 0.6165 - val_acc: 0.7047\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 195us/step - loss: 0.5848 - acc: 0.6751 - val_loss: 0.6278 - val_acc: 0.6929\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 178us/step - loss: 0.5827 - acc: 0.6946 - val_loss: 0.6082 - val_acc: 0.6811\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 0.5724 - acc: 0.6984 - val_loss: 0.5974 - val_acc: 0.7008\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5691 - acc: 0.6926 - val_loss: 0.6094 - val_acc: 0.7008\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 190us/step - loss: 0.5710 - acc: 0.6926 - val_loss: 0.6057 - val_acc: 0.7087\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 188us/step - loss: 0.5653 - acc: 0.6887 - val_loss: 0.6042 - val_acc: 0.6969\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5659 - acc: 0.6887 - val_loss: 0.6074 - val_acc: 0.7126\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 182us/step - loss: 0.5633 - acc: 0.6887 - val_loss: 0.6034 - val_acc: 0.7165\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 179us/step - loss: 0.5622 - acc: 0.6946 - val_loss: 0.6002 - val_acc: 0.6969\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 190us/step - loss: 0.5635 - acc: 0.6887 - val_loss: 0.6402 - val_acc: 0.6654\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 206us/step - loss: 0.5637 - acc: 0.6751 - val_loss: 0.6233 - val_acc: 0.7008\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 188us/step - loss: 0.5635 - acc: 0.6868 - val_loss: 0.6088 - val_acc: 0.7047\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 191us/step - loss: 0.5688 - acc: 0.6868 - val_loss: 0.5909 - val_acc: 0.7165\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5574 - acc: 0.6926 - val_loss: 0.6020 - val_acc: 0.7008\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 183us/step - loss: 0.5596 - acc: 0.6907 - val_loss: 0.6061 - val_acc: 0.6929\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 177us/step - loss: 0.5560 - acc: 0.6907 - val_loss: 0.6057 - val_acc: 0.7126\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 0.5472 - acc: 0.7062 - val_loss: 0.6157 - val_acc: 0.6811\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 174us/step - loss: 0.5574 - acc: 0.6926 - val_loss: 0.6154 - val_acc: 0.6811\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 187us/step - loss: 0.5657 - acc: 0.6926 - val_loss: 0.6236 - val_acc: 0.6890\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 188us/step - loss: 0.5611 - acc: 0.6984 - val_loss: 0.6142 - val_acc: 0.6929\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 213us/step - loss: 0.5526 - acc: 0.6984 - val_loss: 0.6001 - val_acc: 0.6890\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 201us/step - loss: 0.5543 - acc: 0.6965 - val_loss: 0.5938 - val_acc: 0.7047\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 198us/step - loss: 0.5532 - acc: 0.6907 - val_loss: 0.6052 - val_acc: 0.6890\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 0.5536 - acc: 0.7043 - val_loss: 0.5958 - val_acc: 0.7205\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 192us/step - loss: 0.5537 - acc: 0.7004 - val_loss: 0.5945 - val_acc: 0.7087\n"
     ]
    }
   ],
   "source": [
    "#3-layer network with sigmoid activation function\n",
    "model3 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model3.add(Dense(20, input_dim=8, activation='elu'))\n",
    "#create second hidden layer\n",
    "model3.add(Dense(10,activation='sigmoid'))\n",
    "#create third hidden layer\n",
    "model3.add(Dense(5,activation='sigmoid'))\n",
    "#Final Layer\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history3 = model3.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=10, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network with different optimizer and learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Selecting an optimizer](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/)**\n",
    "    - Adam\n",
    "    - SGD\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Learning Rate**\n",
    "\n",
    "    - If you set it too low, training will eventually converge, but it will do so slowly.\n",
    "\n",
    "    - If you set it too high, it might acutally diverge.\n",
    "\n",
    "    - If you set it slightly too high, it will converge at first but miss the local optima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 2ms/step - loss: 0.7844 - acc: 0.6109 - val_loss: 0.7529 - val_acc: 0.3386\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.6602 - acc: 0.6381 - val_loss: 0.6580 - val_acc: 0.6614\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 50us/step - loss: 0.6551 - acc: 0.6459 - val_loss: 0.6422 - val_acc: 0.6614\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.6712 - acc: 0.6459 - val_loss: 0.6506 - val_acc: 0.6614\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.6567 - acc: 0.6459 - val_loss: 0.6405 - val_acc: 0.6614\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 46us/step - loss: 0.6545 - acc: 0.6459 - val_loss: 0.6407 - val_acc: 0.6614\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6568 - acc: 0.6459 - val_loss: 0.6435 - val_acc: 0.6614\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 48us/step - loss: 0.6531 - acc: 0.6459 - val_loss: 0.6577 - val_acc: 0.6614\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.6590 - acc: 0.6459 - val_loss: 0.6501 - val_acc: 0.6614\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.6550 - acc: 0.6459 - val_loss: 0.6473 - val_acc: 0.6614\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6531 - acc: 0.6459 - val_loss: 0.6408 - val_acc: 0.6614\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6548 - acc: 0.6459 - val_loss: 0.6639 - val_acc: 0.6614\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6528 - acc: 0.6459 - val_loss: 0.7056 - val_acc: 0.3386\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6571 - acc: 0.6187 - val_loss: 0.6537 - val_acc: 0.6614\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6499 - acc: 0.6459 - val_loss: 0.6434 - val_acc: 0.6614\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.6529 - acc: 0.6459 - val_loss: 0.6475 - val_acc: 0.6614\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.6538 - acc: 0.6459 - val_loss: 0.6449 - val_acc: 0.6614\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6533 - acc: 0.6459 - val_loss: 0.6435 - val_acc: 0.6614\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.6580 - acc: 0.6459 - val_loss: 0.6434 - val_acc: 0.6614\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6545 - acc: 0.6459 - val_loss: 0.6401 - val_acc: 0.6614\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 38us/step - loss: 0.6521 - acc: 0.6459 - val_loss: 0.6509 - val_acc: 0.6614\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6579 - acc: 0.6459 - val_loss: 0.6545 - val_acc: 0.6614\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.6529 - acc: 0.6459 - val_loss: 0.6427 - val_acc: 0.6614\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6521 - acc: 0.6459 - val_loss: 0.6716 - val_acc: 0.6614\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.6593 - acc: 0.6459 - val_loss: 0.6485 - val_acc: 0.6614\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.6516 - acc: 0.6459 - val_loss: 0.6402 - val_acc: 0.6614\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.6535 - acc: 0.6459 - val_loss: 0.6416 - val_acc: 0.6614\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6539 - acc: 0.6459 - val_loss: 0.6453 - val_acc: 0.6614\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 43us/step - loss: 0.6554 - acc: 0.6459 - val_loss: 0.6519 - val_acc: 0.6614\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.6556 - acc: 0.6459 - val_loss: 0.6502 - val_acc: 0.6614\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6569 - acc: 0.6459 - val_loss: 0.6410 - val_acc: 0.6614\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6549 - acc: 0.6459 - val_loss: 0.6636 - val_acc: 0.6614\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6559 - acc: 0.6459 - val_loss: 0.6496 - val_acc: 0.6614\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6533 - acc: 0.6459 - val_loss: 0.6405 - val_acc: 0.6614\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6583 - acc: 0.6459 - val_loss: 0.6447 - val_acc: 0.6614\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 47us/step - loss: 0.6571 - acc: 0.6459 - val_loss: 0.6492 - val_acc: 0.6614\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.6510 - acc: 0.6459 - val_loss: 0.6673 - val_acc: 0.6614\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 45us/step - loss: 0.6579 - acc: 0.6459 - val_loss: 0.6411 - val_acc: 0.6614\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 36us/step - loss: 0.6569 - acc: 0.6459 - val_loss: 0.6407 - val_acc: 0.6614\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 49us/step - loss: 0.6532 - acc: 0.6459 - val_loss: 0.6426 - val_acc: 0.6614\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6532 - acc: 0.6459 - val_loss: 0.6424 - val_acc: 0.6614\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 44us/step - loss: 0.6548 - acc: 0.6459 - val_loss: 0.6430 - val_acc: 0.6614\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 37us/step - loss: 0.6578 - acc: 0.6459 - val_loss: 0.6402 - val_acc: 0.6614\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.6506 - acc: 0.6459 - val_loss: 0.6555 - val_acc: 0.6614\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 41us/step - loss: 0.6556 - acc: 0.6459 - val_loss: 0.6498 - val_acc: 0.6614\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 35us/step - loss: 0.6537 - acc: 0.6459 - val_loss: 0.6405 - val_acc: 0.6614\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 39us/step - loss: 0.6533 - acc: 0.6459 - val_loss: 0.6465 - val_acc: 0.6614\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 42us/step - loss: 0.6575 - acc: 0.6459 - val_loss: 0.6405 - val_acc: 0.6614\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 40us/step - loss: 0.6521 - acc: 0.6459 - val_loss: 0.6458 - val_acc: 0.6614\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 34us/step - loss: 0.6514 - acc: 0.6459 - val_loss: 0.6416 - val_acc: 0.6614\n"
     ]
    }
   ],
   "source": [
    "#2-layer network with rmsprop optimizer and a learning rate of .1\n",
    "model4 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model4.add(Dense(10, input_dim=8, activation='sigmoid'))\n",
    "#create second hidden layer\n",
    "model4.add(Dense(10,activation='sigmoid'))\n",
    "#Final Layer\n",
    "model4.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.1, rho=0.9, epsilon=None, decay=0.0)\n",
    "model4.compile(loss='binary_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history4 = model4.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens when you change the optimizer and learning rates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Regularization\n",
    "    - L1 and L2\n",
    "\n",
    "    - Dropout \n",
    "        \n",
    "        The most popular techniqure for deep neural networks. It is a fairly simple algorithm where at every training step, every neuron has a probability fo being teporarily \"droppedout,\" meaning it will be completely ignored dureing this traing step, but it may be active during the next step.\n",
    "    \n",
    "    - [Early Stopping](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "    \n",
    "   Just interrupt training when its performance on the validation set starts dropping\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Paper on selecting hyperparameters](https://arxiv.org/pdf/1206.5533v2.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 3ms/step - loss: 2.5195 - acc: 0.5584 - val_loss: 0.8876 - val_acc: 0.6535\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 73us/step - loss: 2.4412 - acc: 0.5700 - val_loss: 0.8428 - val_acc: 0.6535\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 59us/step - loss: 2.0393 - acc: 0.5642 - val_loss: 0.8307 - val_acc: 0.6614\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 1.6243 - acc: 0.5895 - val_loss: 0.8263 - val_acc: 0.6614\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 1.5180 - acc: 0.6031 - val_loss: 0.8220 - val_acc: 0.6614\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 59us/step - loss: 1.5141 - acc: 0.6128 - val_loss: 0.8179 - val_acc: 0.6614\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 1.3854 - acc: 0.6245 - val_loss: 0.8142 - val_acc: 0.6614\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 62us/step - loss: 1.4712 - acc: 0.5895 - val_loss: 0.8105 - val_acc: 0.6614\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 59us/step - loss: 1.3154 - acc: 0.6070 - val_loss: 0.8068 - val_acc: 0.6614\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 1.3162 - acc: 0.5934 - val_loss: 0.8033 - val_acc: 0.6614\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 67us/step - loss: 1.1233 - acc: 0.6148 - val_loss: 0.8001 - val_acc: 0.6614\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 64us/step - loss: 1.1234 - acc: 0.6245 - val_loss: 0.7970 - val_acc: 0.6614\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 63us/step - loss: 1.1039 - acc: 0.6284 - val_loss: 0.7939 - val_acc: 0.6614\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 57us/step - loss: 1.1822 - acc: 0.6187 - val_loss: 0.7910 - val_acc: 0.6614\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 62us/step - loss: 1.0222 - acc: 0.6089 - val_loss: 0.7880 - val_acc: 0.6614\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 59us/step - loss: 0.9296 - acc: 0.6420 - val_loss: 0.7852 - val_acc: 0.6614\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.9944 - acc: 0.6265 - val_loss: 0.7825 - val_acc: 0.6614\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.8741 - acc: 0.6420 - val_loss: 0.7800 - val_acc: 0.6614\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 64us/step - loss: 1.0108 - acc: 0.6304 - val_loss: 0.7774 - val_acc: 0.6614\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.9158 - acc: 0.6304 - val_loss: 0.7748 - val_acc: 0.6614\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.9121 - acc: 0.6167 - val_loss: 0.7724 - val_acc: 0.6614\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.8686 - acc: 0.6479 - val_loss: 0.7702 - val_acc: 0.6614\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 71us/step - loss: 0.9174 - acc: 0.6226 - val_loss: 0.7682 - val_acc: 0.6614\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.8967 - acc: 0.6342 - val_loss: 0.7662 - val_acc: 0.6614\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.8141 - acc: 0.6284 - val_loss: 0.7642 - val_acc: 0.6614\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.8091 - acc: 0.6440 - val_loss: 0.7621 - val_acc: 0.6614\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.8619 - acc: 0.6342 - val_loss: 0.7601 - val_acc: 0.6614\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 66us/step - loss: 0.8028 - acc: 0.6304 - val_loss: 0.7582 - val_acc: 0.6614\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.7911 - acc: 0.6459 - val_loss: 0.7564 - val_acc: 0.6614\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.8505 - acc: 0.6342 - val_loss: 0.7545 - val_acc: 0.6614\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 55us/step - loss: 0.8126 - acc: 0.6323 - val_loss: 0.7525 - val_acc: 0.6614\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.8121 - acc: 0.6381 - val_loss: 0.7508 - val_acc: 0.6614\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 61us/step - loss: 0.7803 - acc: 0.6420 - val_loss: 0.7492 - val_acc: 0.6614\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 64us/step - loss: 0.7949 - acc: 0.6381 - val_loss: 0.7477 - val_acc: 0.6614\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 69us/step - loss: 0.7726 - acc: 0.6420 - val_loss: 0.7460 - val_acc: 0.6614\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 63us/step - loss: 0.7671 - acc: 0.6556 - val_loss: 0.7444 - val_acc: 0.6614\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.7560 - acc: 0.6459 - val_loss: 0.7430 - val_acc: 0.6614\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.7988 - acc: 0.6362 - val_loss: 0.7414 - val_acc: 0.6614\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 56us/step - loss: 0.7867 - acc: 0.6323 - val_loss: 0.7400 - val_acc: 0.6614\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.7882 - acc: 0.6381 - val_loss: 0.7385 - val_acc: 0.6614\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 65us/step - loss: 0.7695 - acc: 0.6420 - val_loss: 0.7372 - val_acc: 0.6614\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 235us/step - loss: 0.7489 - acc: 0.6381 - val_loss: 0.7359 - val_acc: 0.6614\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.7529 - acc: 0.6459 - val_loss: 0.7346 - val_acc: 0.6614\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 67us/step - loss: 0.7354 - acc: 0.6498 - val_loss: 0.7333 - val_acc: 0.6614\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 58us/step - loss: 0.7497 - acc: 0.6381 - val_loss: 0.7321 - val_acc: 0.6614\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 60us/step - loss: 0.7801 - acc: 0.6304 - val_loss: 0.7308 - val_acc: 0.6614\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 68us/step - loss: 0.7459 - acc: 0.6362 - val_loss: 0.7295 - val_acc: 0.6614\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 62us/step - loss: 0.7427 - acc: 0.6420 - val_loss: 0.7284 - val_acc: 0.6614\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 57us/step - loss: 0.7565 - acc: 0.6420 - val_loss: 0.7273 - val_acc: 0.6614\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 52us/step - loss: 0.7593 - acc: 0.6342 - val_loss: 0.7261 - val_acc: 0.6614\n"
     ]
    }
   ],
   "source": [
    "#3-layer network with sigmoid activation function\n",
    "model5 = Sequential()\n",
    "\n",
    "#create first hidden layer\n",
    "model5.add(Dense(16, input_dim=8, activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "model5.add(Dropout(0.50))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "model5.add(Dense(units=8, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "# Add a dropout layer for previous hidden layer\n",
    "model5.add(Dropout(0.5))\n",
    "# Add fully connected layer with a ReLU activation function and L2 regularization\n",
    "model5.add(Dense(units=4, kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "#Final Layer\n",
    "model5.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "#compile model\n",
    "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model5.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#train 2nd model\n",
    "history5 = model5.fit(X_train, # Features\n",
    "                      y_train, # Target\n",
    "                      epochs=50, # Number of epochs\n",
    "                      verbose=1, # Some output\n",
    "                      batch_size=50, # Number of observations per batch\n",
    "                      validation_data=(X_test, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Using GridSearchCV to tune Neural Networks](https://chrisalbon.com/deep_learning/keras/tuning_neural_network_hyperparameters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Network Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda install -c anaconda pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"543pt\" viewBox=\"0.00 0.00 269.00 543.00\" width=\"269pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 539)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-539 265,-539 265,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 112913711904 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>112913711904</title>\n",
       "<polygon fill=\"none\" points=\"8,-415.5 8,-461.5 253,-461.5 253,-415.5 8,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-434.8\">dense_20: Dense</text>\n",
       "<polyline fill=\"none\" points=\"120,-415.5 120,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120,-438.5 176,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"176,-415.5 176,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-446.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"176,-438.5 253,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-423.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 112913712408 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>112913712408</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 261,-378.5 261,-332.5 0,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-351.8\">dropout_5: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"128,-332.5 128,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"128,-355.5 184,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"156\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"184,-332.5 184,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-363.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"184,-355.5 261,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-340.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 112913711904&#45;&gt;112913712408 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>112913711904-&gt;112913712408</title>\n",
       "<path d=\"M130.5,-415.3799C130.5,-407.1745 130.5,-397.7679 130.5,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-388.784 130.5,-378.784 127.0001,-388.784 134.0001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112913712800 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112913712800</title>\n",
       "<polygon fill=\"none\" points=\"8,-249.5 8,-295.5 253,-295.5 253,-249.5 8,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"64\" y=\"-268.8\">dense_21: Dense</text>\n",
       "<polyline fill=\"none\" points=\"120,-249.5 120,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"120,-272.5 176,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"176,-249.5 176,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-280.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"176,-272.5 253,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-257.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 112913712408&#45;&gt;112913712800 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112913712408-&gt;112913712800</title>\n",
       "<path d=\"M130.5,-332.3799C130.5,-324.1745 130.5,-314.7679 130.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-305.784 130.5,-295.784 127.0001,-305.784 134.0001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112913712520 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>112913712520</title>\n",
       "<polygon fill=\"none\" points=\"3.5,-166.5 3.5,-212.5 257.5,-212.5 257.5,-166.5 3.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.5\" y=\"-185.8\">dropout_6: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-166.5 131.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131.5,-189.5 187.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-166.5 187.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-197.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-189.5 257.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-174.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 112913712800&#45;&gt;112913712520 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>112913712800-&gt;112913712520</title>\n",
       "<path d=\"M130.5,-249.3799C130.5,-241.1745 130.5,-231.7679 130.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-222.784 130.5,-212.784 127.0001,-222.784 134.0001,-222.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112913712296 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>112913712296</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-83.5 11.5,-129.5 249.5,-129.5 249.5,-83.5 11.5,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.5\" y=\"-102.8\">dense_22: Dense</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-83.5 123.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-106.5 179.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"179.5,-83.5 179.5,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-114.3\">(None, 8)</text>\n",
       "<polyline fill=\"none\" points=\"179.5,-106.5 249.5,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-91.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 112913712520&#45;&gt;112913712296 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>112913712520-&gt;112913712296</title>\n",
       "<path d=\"M130.5,-166.3799C130.5,-158.1745 130.5,-148.7679 130.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-139.784 130.5,-129.784 127.0001,-139.784 134.0001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112913779232 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>112913779232</title>\n",
       "<polygon fill=\"none\" points=\"11.5,-.5 11.5,-46.5 249.5,-46.5 249.5,-.5 11.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67.5\" y=\"-19.8\">dense_23: Dense</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-.5 123.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"123.5,-23.5 179.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"151.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"179.5,-.5 179.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-31.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"179.5,-23.5 249.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"214.5\" y=\"-8.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 112913712296&#45;&gt;112913779232 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>112913712296-&gt;112913779232</title>\n",
       "<path d=\"M130.5,-83.3799C130.5,-75.1745 130.5,-65.7679 130.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-56.784 130.5,-46.784 127.0001,-56.784 134.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112913777552 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>112913777552</title>\n",
       "<polygon fill=\"none\" points=\"80,-498.5 80,-534.5 181,-534.5 181,-498.5 80,-498.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"130.5\" y=\"-512.8\">112913777552</text>\n",
       "</g>\n",
       "<!-- 112913777552&#45;&gt;112913711904 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>112913777552-&gt;112913711904</title>\n",
       "<path d=\"M130.5,-498.4092C130.5,-490.4308 130.5,-480.795 130.5,-471.606\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"134.0001,-471.5333 130.5,-461.5333 127.0001,-471.5334 134.0001,-471.5333\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "#Visualize network architecture\n",
    "SVG(model_to_dot(model5, show_shapes=True).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization as a file\n",
    "plot_model(model5, show_shapes=True, to_file='network.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_neural_network_architecture/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras Implementation of optimizers](https://keras.io/optimizers/)\n",
    "\n",
    "[Impact of Learning Rate on Model Performance](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755],\n",
       "       [0.4075755]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model5.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history5.history['loss']\n",
    "test_loss = history5.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_loss_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bb54cf484908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Visualize accuracy history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Get training and test accuracy histories\n",
    "training_accuracy = history5.history['acc']\n",
    "test_accuracy = history5.history['val_acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "# Visualize accuracy history\n",
    "plt.plot(epoch_count, training_accuracy, 'r--')\n",
    "plt.plot(epoch_count, test_accuracy, 'b-')\n",
    "plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/deep_learning/keras/visualize_performance_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model5.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/\n",
    "    \n",
    "http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\n",
    "\n",
    "https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
