{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/swilson5/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n",
      " 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n",
      " 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n",
      " 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n",
      " 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n",
      " 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
       "      <td>6</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  target  \\\n",
       "0     From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1     From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "10    From: irwin@cmptrc.lonestar.org (Irwin Arnstei...       8   \n",
       "100   From: tchen@magnus.acs.ohio-state.edu (Tsung-K...       6   \n",
       "1000  From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...       2   \n",
       "\n",
       "                 target_names  \n",
       "0                   rec.autos  \n",
       "1       comp.sys.mac.hardware  \n",
       "10            rec.motorcycles  \n",
       "100              misc.forsale  \n",
       "1000  comp.os.ms-windows.misc  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import Dataset\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.machinelearningplus.com/wp-content/uploads/2018/03/Inferring-Topic-from-Keywords-1024x666.png' img/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d8b43408e7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "WordNetLemmatizer().lemmatize(text, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    word = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "#     print('token',word)\n",
    "    return stemmer.stem(word)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)\\nSubject:', 'WHAT', 'car', 'is', 'this!?\\nNntp-Posting-Host:', 'rac3.wam.umd.edu\\nOrganization:', 'University', 'of', 'Maryland,', 'College', 'Park\\nLines:', '15\\n\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\nthe', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\nearly', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', '\\nall', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years\\nof', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you\\nhave', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.\\n\\nThanks,\\n-', 'IL\\n', '', '', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----\\n\\n\\n\\n\\n']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = df.iloc[0].values[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lerxst, thing, subject, nntp, post, host, org...\n",
       "1        [guykuo, carson, washington, subject, clock, p...\n",
       "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
       "100      [tchen, magnus, ohio, state, tsung, chen, subj...\n",
       "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
       "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
       "10001    [kimman, magnus, ohio, state, richard, subject...\n",
       "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
       "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
       "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = df['content'].map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lerxst, thing, subject, nntp, post, host, org...\n",
       "1        [guykuo, carson, washington, subject, clock, p...\n",
       "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
       "100      [tchen, magnus, ohio, state, tsung, chen, subj...\n",
       "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
       "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
       "10001    [kimman, magnus, ohio, state, richard, subject...\n",
       "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
       "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
       "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
       "10005    [dsto, david, silver, subject, fractal, genera...\n",
       "10006    [subject, mike, francesa, predict, gajarski, p...\n",
       "10007    [netcom, netcom, eric, townsend, subject, insu...\n",
       "10008    [cunixb, columbia, gari, dare, subject, covera...\n",
       "10009    [sehari, iastat, babak, sehari, subject, disk,...\n",
       "1001     [danmg, grok, columbiasc, daniel, adam, subjec...\n",
       "10010    [henri, toronto, henri, spencer, subject, luna...\n",
       "10011    [stein, washington, smith, subject, tie, abort...\n",
       "10012    [uicvm, subject, lciii, midi, articl, uicvm, o...\n",
       "10013    [nsmca, aurora, alaska, subject, lunar, coloni...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words on the Data set\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out tokens that appear in:\n",
    "\n",
    "- less than 15 documents (absolute number) \n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 1),\n",
       " (20, 1),\n",
       " (26, 1),\n",
       " (30, 1),\n",
       " (138, 1),\n",
       " (142, 3),\n",
       " (244, 1),\n",
       " (271, 5),\n",
       " (403, 2),\n",
       " (593, 1),\n",
       " (598, 1),\n",
       " (630, 1),\n",
       " (657, 1),\n",
       " (701, 1),\n",
       " (815, 1),\n",
       " (836, 1),\n",
       " (877, 1),\n",
       " (921, 1),\n",
       " (985, 1),\n",
       " (1046, 1),\n",
       " (1388, 1),\n",
       " (1397, 1),\n",
       " (1568, 1),\n",
       " (1617, 1),\n",
       " (1685, 1),\n",
       " (1839, 2),\n",
       " (1897, 1),\n",
       " (2004, 1),\n",
       " (2502, 1),\n",
       " (2662, 1),\n",
       " (2709, 1),\n",
       " (2848, 1),\n",
       " (2958, 1),\n",
       " (3304, 1),\n",
       " (4584, 1),\n",
       " (4807, 1),\n",
       " (5080, 2),\n",
       " (5764, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 11 (\"host\") appears 1 time.\n",
      "Word 20 (\"nntp\") appears 1 time.\n",
      "Word 26 (\"spec\") appears 1 time.\n",
      "Word 30 (\"univers\") appears 1 time.\n",
      "Word 138 (\"state\") appears 1 time.\n",
      "Word 142 (\"window\") appears 3 time.\n",
      "Word 244 (\"richard\") appears 1 time.\n",
      "Word 271 (\"program\") appears 5 time.\n",
      "Word 403 (\"silver\") appears 2 time.\n",
      "Word 593 (\"secur\") appears 1 time.\n",
      "Word 598 (\"true\") appears 1 time.\n",
      "Word 630 (\"gate\") appears 1 time.\n",
      "Word 657 (\"econom\") appears 1 time.\n",
      "Word 701 (\"high\") appears 1 time.\n",
      "Word 815 (\"support\") appears 1 time.\n",
      "Word 836 (\"meet\") appears 1 time.\n",
      "Word 877 (\"correct\") appears 1 time.\n",
      "Word 921 (\"task\") appears 1 time.\n",
      "Word 985 (\"current\") appears 1 time.\n",
      "Word 1046 (\"major\") appears 1 time.\n",
      "Word 1388 (\"oper\") appears 1 time.\n",
      "Word 1397 (\"promis\") appears 1 time.\n",
      "Word 1568 (\"dept\") appears 1 time.\n",
      "Word 1617 (\"server\") appears 1 time.\n",
      "Word 1685 (\"user\") appears 1 time.\n",
      "Word 1839 (\"multi\") appears 2 time.\n",
      "Word 1897 (\"expect\") appears 1 time.\n",
      "Word 2004 (\"assur\") appears 1 time.\n",
      "Word 2502 (\"jose\") appears 1 time.\n",
      "Word 2662 (\"processor\") appears 1 time.\n",
      "Word 2709 (\"math\") appears 1 time.\n",
      "Word 2848 (\"rick\") appears 1 time.\n",
      "Word 2958 (\"overhead\") appears 1 time.\n",
      "Word 3304 (\"warner\") appears 1 time.\n",
      "Word 4584 (\"giant\") appears 1 time.\n",
      "Word 4807 (\"primarili\") appears 1 time.\n",
      "Word 5080 (\"billi\") appears 2 time.\n",
      "Word 5764 (\"app\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16531831488632115),\n",
      " (1, 0.1678553823993299),\n",
      " (2, 0.15020052155842978),\n",
      " (3, 0.28581344134222897),\n",
      " (4, 0.11439130765694391),\n",
      " (5, 0.1516798683845623),\n",
      " (6, 0.3893283888350302),\n",
      " (7, 0.16890780449478127),\n",
      " (8, 0.12279484913135752),\n",
      " (9, 0.2634573656114004),\n",
      " (10, 0.16446021701525967),\n",
      " (11, 0.041907566184917734),\n",
      " (12, 0.13943742962367772),\n",
      " (13, 0.053228561863657146),\n",
      " (14, 0.17840678372959912),\n",
      " (15, 0.1614581045972935),\n",
      " (16, 0.10182359643822467),\n",
      " (17, 0.23339500537007382),\n",
      " (18, 0.1622034571062096),\n",
      " (19, 0.28046098400184816),\n",
      " (20, 0.04264750560541665),\n",
      " (21, 0.18494250912032378),\n",
      " (22, 0.14867573439400095),\n",
      " (23, 0.15971285457776704),\n",
      " (24, 0.18156677399111007),\n",
      " (25, 0.14477104789605966),\n",
      " (26, 0.20972103713602588),\n",
      " (27, 0.19290536120043997),\n",
      " (28, 0.08432238906696132),\n",
      " (29, 0.08377240739564006),\n",
      " (30, 0.04441833427243015),\n",
      " (31, 0.13721646110054028),\n",
      " (32, 0.08417832496773713)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.010*\"think\" + 0.009*\"peopl\" + 0.008*\"christian\" + 0.007*\"know\" + 0.007*\"jesus\" + 0.006*\"believ\" + 0.006*\"like\" + 0.006*\"articl\" + 0.005*\"say\" + 0.005*\"come\"\n",
      "Topic: 1 \n",
      "Words: 0.009*\"drive\" + 0.008*\"articl\" + 0.006*\"control\" + 0.006*\"univers\" + 0.006*\"like\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"disk\" + 0.004*\"think\" + 0.004*\"know\"\n",
      "Topic: 2 \n",
      "Words: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.006*\"secur\" + 0.006*\"clipper\" + 0.006*\"govern\" + 0.006*\"inform\" + 0.005*\"technolog\" + 0.005*\"time\" + 0.005*\"escrow\" + 0.005*\"know\"\n",
      "Topic: 3 \n",
      "Words: 0.009*\"say\" + 0.008*\"peopl\" + 0.007*\"come\" + 0.007*\"know\" + 0.007*\"armenian\" + 0.006*\"go\" + 0.006*\"year\" + 0.005*\"like\" + 0.005*\"think\" + 0.005*\"time\"\n",
      "Topic: 4 \n",
      "Words: 0.007*\"peopl\" + 0.007*\"articl\" + 0.005*\"think\" + 0.005*\"know\" + 0.005*\"like\" + 0.005*\"time\" + 0.004*\"pitt\" + 0.004*\"scienc\" + 0.004*\"univers\" + 0.004*\"say\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"team\" + 0.010*\"year\" + 0.009*\"game\" + 0.008*\"play\" + 0.008*\"player\" + 0.007*\"think\" + 0.006*\"season\" + 0.006*\"like\" + 0.006*\"know\" + 0.006*\"time\"\n",
      "Topic: 6 \n",
      "Words: 0.019*\"window\" + 0.009*\"like\" + 0.008*\"articl\" + 0.007*\"think\" + 0.006*\"time\" + 0.005*\"driver\" + 0.005*\"know\" + 0.005*\"univers\" + 0.004*\"work\" + 0.004*\"host\"\n",
      "Topic: 7 \n",
      "Words: 0.008*\"right\" + 0.008*\"articl\" + 0.007*\"time\" + 0.007*\"univers\" + 0.006*\"think\" + 0.006*\"islam\" + 0.006*\"state\" + 0.005*\"turkish\" + 0.005*\"greek\" + 0.005*\"peopl\"\n",
      "Topic: 8 \n",
      "Words: 0.010*\"wire\" + 0.007*\"articl\" + 0.006*\"state\" + 0.005*\"time\" + 0.004*\"univers\" + 0.004*\"like\" + 0.004*\"grind\" + 0.004*\"power\" + 0.004*\"good\" + 0.004*\"ohio\"\n",
      "Topic: 9 \n",
      "Words: 0.008*\"file\" + 0.007*\"like\" + 0.006*\"entri\" + 0.006*\"articl\" + 0.005*\"mail\" + 0.005*\"peopl\" + 0.005*\"clipper\" + 0.005*\"encrypt\" + 0.005*\"host\" + 0.005*\"univers\"\n",
      "Topic: 10 \n",
      "Words: 0.010*\"know\" + 0.009*\"peopl\" + 0.006*\"say\" + 0.005*\"think\" + 0.005*\"like\" + 0.005*\"articl\" + 0.005*\"mean\" + 0.005*\"go\" + 0.005*\"work\" + 0.005*\"presid\"\n",
      "Topic: 11 \n",
      "Words: 0.010*\"articl\" + 0.008*\"sandvik\" + 0.007*\"univers\" + 0.006*\"christian\" + 0.006*\"appl\" + 0.005*\"kent\" + 0.004*\"state\" + 0.004*\"peopl\" + 0.004*\"book\" + 0.004*\"newton\"\n",
      "Topic: 12 \n",
      "Words: 0.007*\"articl\" + 0.007*\"like\" + 0.007*\"know\" + 0.006*\"univers\" + 0.005*\"say\" + 0.005*\"think\" + 0.005*\"host\" + 0.005*\"work\" + 0.005*\"toronto\" + 0.004*\"nntp\"\n",
      "Topic: 13 \n",
      "Words: 0.011*\"nntp\" + 0.011*\"host\" + 0.010*\"articl\" + 0.007*\"think\" + 0.006*\"john\" + 0.006*\"univers\" + 0.005*\"access\" + 0.005*\"say\" + 0.004*\"repli\" + 0.004*\"bike\"\n",
      "Topic: 14 \n",
      "Words: 0.012*\"card\" + 0.008*\"articl\" + 0.008*\"know\" + 0.007*\"univers\" + 0.007*\"like\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.006*\"good\" + 0.005*\"work\" + 0.005*\"problem\"\n",
      "Topic: 15 \n",
      "Words: 0.011*\"univers\" + 0.009*\"host\" + 0.009*\"nntp\" + 0.008*\"space\" + 0.007*\"nasa\" + 0.006*\"know\" + 0.006*\"like\" + 0.006*\"articl\" + 0.006*\"thank\" + 0.005*\"need\"\n",
      "Topic: 16 \n",
      "Words: 0.017*\"drive\" + 0.014*\"scsi\" + 0.010*\"articl\" + 0.008*\"nntp\" + 0.008*\"host\" + 0.007*\"problem\" + 0.006*\"univers\" + 0.006*\"like\" + 0.005*\"work\" + 0.005*\"know\"\n",
      "Topic: 17 \n",
      "Words: 0.008*\"israel\" + 0.008*\"articl\" + 0.007*\"know\" + 0.007*\"isra\" + 0.007*\"think\" + 0.005*\"time\" + 0.005*\"like\" + 0.005*\"good\" + 0.005*\"say\" + 0.004*\"work\"\n",
      "Topic: 18 \n",
      "Words: 0.009*\"armenian\" + 0.008*\"file\" + 0.007*\"program\" + 0.007*\"imag\" + 0.006*\"avail\" + 0.006*\"window\" + 0.005*\"data\" + 0.005*\"includ\" + 0.005*\"softwar\" + 0.004*\"graphic\"\n",
      "Topic: 19 \n",
      "Words: 0.009*\"file\" + 0.008*\"like\" + 0.006*\"know\" + 0.006*\"articl\" + 0.005*\"peopl\" + 0.005*\"includ\" + 0.004*\"space\" + 0.004*\"sourc\" + 0.004*\"drug\" + 0.004*\"go\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"jake\" + 0.002*\"boni\" + 0.002*\"jew\" + 0.002*\"team\" + 0.002*\"file\" + 0.002*\"game\" + 0.002*\"israel\" + 0.002*\"year\" + 0.002*\"arab\" + 0.002*\"okcforum\"\n",
      "Topic: 1 Word: 0.004*\"ohio\" + 0.004*\"encrypt\" + 0.004*\"chip\" + 0.003*\"clipper\" + 0.003*\"escrow\" + 0.003*\"henri\" + 0.003*\"state\" + 0.002*\"key\" + 0.002*\"govern\" + 0.002*\"magnus\"\n",
      "Topic: 2 Word: 0.002*\"access\" + 0.002*\"food\" + 0.002*\"nasa\" + 0.002*\"know\" + 0.002*\"world\" + 0.002*\"digex\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"baalk\" + 0.002*\"scsi\"\n",
      "Topic: 3 Word: 0.007*\"sandvik\" + 0.004*\"kent\" + 0.004*\"newton\" + 0.003*\"simm\" + 0.003*\"appl\" + 0.002*\"buffalo\" + 0.002*\"kurt\" + 0.002*\"thank\" + 0.002*\"group\" + 0.002*\"chip\"\n",
      "Topic: 4 Word: 0.003*\"isra\" + 0.003*\"israel\" + 0.003*\"buffalo\" + 0.002*\"arab\" + 0.002*\"team\" + 0.002*\"think\" + 0.002*\"articl\" + 0.002*\"peopl\" + 0.002*\"church\" + 0.002*\"game\"\n",
      "Topic: 5 Word: 0.002*\"colorado\" + 0.002*\"mail\" + 0.002*\"upenn\" + 0.002*\"christian\" + 0.002*\"andrew\" + 0.002*\"thank\" + 0.002*\"file\" + 0.002*\"peopl\" + 0.002*\"univers\" + 0.002*\"articl\"\n",
      "Topic: 6 Word: 0.002*\"peopl\" + 0.002*\"think\" + 0.002*\"know\" + 0.002*\"year\" + 0.002*\"christian\" + 0.002*\"say\" + 0.002*\"like\" + 0.002*\"game\" + 0.002*\"time\" + 0.002*\"come\"\n",
      "Topic: 7 Word: 0.003*\"keith\" + 0.002*\"livesey\" + 0.002*\"caltech\" + 0.002*\"joystick\" + 0.002*\"moral\" + 0.002*\"distribut\" + 0.002*\"game\" + 0.002*\"univers\" + 0.002*\"year\" + 0.002*\"solntz\"\n",
      "Topic: 8 Word: 0.007*\"window\" + 0.004*\"mous\" + 0.004*\"monitor\" + 0.002*\"screen\" + 0.002*\"card\" + 0.002*\"modem\" + 0.002*\"univers\" + 0.002*\"problem\" + 0.002*\"thank\" + 0.002*\"host\"\n",
      "Topic: 9 Word: 0.003*\"pitt\" + 0.003*\"gordon\" + 0.002*\"pwiseman\" + 0.002*\"bank\" + 0.002*\"nasa\" + 0.002*\"cliff\" + 0.002*\"mike\" + 0.002*\"salmon\" + 0.002*\"persian\" + 0.002*\"articl\"\n",
      "Topic: 10 Word: 0.003*\"drive\" + 0.002*\"control\" + 0.002*\"sale\" + 0.002*\"disk\" + 0.002*\"card\" + 0.002*\"hard\" + 0.002*\"harri\" + 0.002*\"good\" + 0.002*\"like\" + 0.002*\"engin\"\n",
      "Topic: 11 Word: 0.004*\"window\" + 0.003*\"card\" + 0.003*\"server\" + 0.003*\"dresden\" + 0.002*\"mode\" + 0.002*\"scsi\" + 0.002*\"austin\" + 0.002*\"oracl\" + 0.002*\"printer\" + 0.002*\"motif\"\n",
      "Topic: 12 Word: 0.003*\"armenian\" + 0.002*\"pitt\" + 0.002*\"time\" + 0.002*\"gordon\" + 0.002*\"bank\" + 0.002*\"know\" + 0.002*\"window\" + 0.002*\"number\" + 0.002*\"peopl\" + 0.002*\"year\"\n",
      "Topic: 13 Word: 0.003*\"duke\" + 0.003*\"drive\" + 0.003*\"card\" + 0.002*\"princeton\" + 0.002*\"window\" + 0.002*\"univers\" + 0.002*\"thank\" + 0.002*\"ncsu\" + 0.002*\"like\" + 0.002*\"higgin\"\n",
      "Topic: 14 Word: 0.002*\"bike\" + 0.002*\"columbia\" + 0.002*\"cunixb\" + 0.002*\"know\" + 0.002*\"helmet\" + 0.002*\"insur\" + 0.002*\"good\" + 0.002*\"team\" + 0.002*\"univers\" + 0.002*\"armenian\"\n",
      "Topic: 15 Word: 0.002*\"engr\" + 0.002*\"vnet\" + 0.002*\"speedi\" + 0.002*\"latech\" + 0.002*\"peopl\" + 0.002*\"engin\" + 0.002*\"univers\" + 0.002*\"mike\" + 0.002*\"articl\" + 0.002*\"uiuc\"\n",
      "Topic: 16 Word: 0.003*\"window\" + 0.003*\"file\" + 0.002*\"disk\" + 0.002*\"know\" + 0.002*\"simm\" + 0.002*\"like\" + 0.002*\"univers\" + 0.002*\"peopl\" + 0.002*\"imag\" + 0.002*\"need\"\n",
      "Topic: 17 Word: 0.005*\"window\" + 0.003*\"file\" + 0.003*\"driver\" + 0.002*\"video\" + 0.002*\"sdsu\" + 0.002*\"card\" + 0.002*\"mous\" + 0.002*\"nist\" + 0.002*\"program\" + 0.002*\"applic\"\n",
      "Topic: 18 Word: 0.004*\"jesus\" + 0.004*\"stratus\" + 0.003*\"christian\" + 0.003*\"peopl\" + 0.002*\"think\" + 0.002*\"believ\" + 0.002*\"faith\" + 0.002*\"know\" + 0.002*\"say\" + 0.002*\"bibl\"\n",
      "Topic: 19 Word: 0.005*\"nasa\" + 0.004*\"alaska\" + 0.004*\"space\" + 0.002*\"mccall\" + 0.002*\"acad\" + 0.002*\"nsmca\" + 0.002*\"problem\" + 0.002*\"aurora\" + 0.002*\"gsfc\" + 0.002*\"thank\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=20, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'subject',\n",
       " 'hard',\n",
       " 'drive',\n",
       " 'secur',\n",
       " 'target',\n",
       " 'origin',\n",
       " 'gtoal',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'keyword',\n",
       " 'entropi',\n",
       " 'nntp',\n",
       " 'post',\n",
       " 'host',\n",
       " 'pizzabox',\n",
       " 'demon',\n",
       " 'repli',\n",
       " 'graham',\n",
       " 'toal',\n",
       " 'gtoal',\n",
       " 'gtoal',\n",
       " 'organ',\n",
       " 'cuddlehog',\n",
       " 'anonym',\n",
       " 'line',\n",
       " 'articl',\n",
       " 'kean',\n",
       " 'write',\n",
       " 'matter',\n",
       " 'fact',\n",
       " 'random',\n",
       " 'file',\n",
       " 'disk',\n",
       " 'reason',\n",
       " 'special',\n",
       " 'purpos',\n",
       " 'hardwar',\n",
       " 'take',\n",
       " 'long',\n",
       " 'time',\n",
       " 'generat',\n",
       " 'good',\n",
       " 'random',\n",
       " 'bit',\n",
       " 'program',\n",
       " 'crank',\n",
       " 'coupl',\n",
       " 'bit',\n",
       " 'minut',\n",
       " 'pretti',\n",
       " 'conserv',\n",
       " 'time',\n",
       " 'need',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'use',\n",
       " 'program',\n",
       " 'interest',\n",
       " 'post',\n",
       " 'sourc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.3699028789997101\t \n",
      "Topic: 0.019*\"window\" + 0.009*\"like\" + 0.008*\"articl\" + 0.007*\"think\" + 0.006*\"time\" + 0.005*\"driver\" + 0.005*\"know\" + 0.005*\"univers\" + 0.004*\"work\" + 0.004*\"host\"\n",
      "\n",
      "Score: 0.35268667340278625\t \n",
      "Topic: 0.011*\"univers\" + 0.009*\"host\" + 0.009*\"nntp\" + 0.008*\"space\" + 0.007*\"nasa\" + 0.006*\"know\" + 0.006*\"like\" + 0.006*\"articl\" + 0.006*\"thank\" + 0.005*\"need\"\n",
      "\n",
      "Score: 0.2597021162509918\t \n",
      "Topic: 0.009*\"drive\" + 0.008*\"articl\" + 0.006*\"control\" + 0.006*\"univers\" + 0.006*\"like\" + 0.005*\"peopl\" + 0.004*\"year\" + 0.004*\"disk\" + 0.004*\"think\" + 0.004*\"know\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5702365040779114\t \n",
      "Topic: 0.003*\"armenian\" + 0.002*\"pitt\" + 0.002*\"time\" + 0.002*\"gordon\" + 0.002*\"bank\" + 0.002*\"know\" + 0.002*\"window\" + 0.002*\"number\" + 0.002*\"peopl\" + 0.002*\"year\"\n",
      "\n",
      "Score: 0.26332250237464905\t \n",
      "Topic: 0.004*\"window\" + 0.003*\"card\" + 0.003*\"server\" + 0.003*\"dresden\" + 0.002*\"mode\" + 0.002*\"scsi\" + 0.002*\"austin\" + 0.002*\"oracl\" + 0.002*\"printer\" + 0.002*\"motif\"\n",
      "\n",
      "Score: 0.14873264729976654\t \n",
      "Topic: 0.007*\"window\" + 0.004*\"mous\" + 0.004*\"monitor\" + 0.002*\"screen\" + 0.002*\"card\" + 0.002*\"modem\" + 0.002*\"univers\" + 0.002*\"problem\" + 0.002*\"thank\" + 0.002*\"host\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.762499988079071\t Topic: 0.010*\"encrypt\" + 0.009*\"chip\" + 0.006*\"secur\" + 0.006*\"clipper\" + 0.006*\"govern\"\n",
      "Score: 0.012500000186264515\t Topic: 0.010*\"think\" + 0.009*\"peopl\" + 0.008*\"christian\" + 0.007*\"know\" + 0.007*\"jesus\"\n",
      "Score: 0.012500000186264515\t Topic: 0.009*\"drive\" + 0.008*\"articl\" + 0.006*\"control\" + 0.006*\"univers\" + 0.006*\"like\"\n",
      "Score: 0.012500000186264515\t Topic: 0.009*\"say\" + 0.008*\"peopl\" + 0.007*\"come\" + 0.007*\"know\" + 0.007*\"armenian\"\n",
      "Score: 0.012500000186264515\t Topic: 0.007*\"peopl\" + 0.007*\"articl\" + 0.005*\"think\" + 0.005*\"know\" + 0.005*\"like\"\n",
      "Score: 0.012500000186264515\t Topic: 0.011*\"team\" + 0.010*\"year\" + 0.009*\"game\" + 0.008*\"play\" + 0.008*\"player\"\n",
      "Score: 0.012500000186264515\t Topic: 0.019*\"window\" + 0.009*\"like\" + 0.008*\"articl\" + 0.007*\"think\" + 0.006*\"time\"\n",
      "Score: 0.012500000186264515\t Topic: 0.008*\"right\" + 0.008*\"articl\" + 0.007*\"time\" + 0.007*\"univers\" + 0.006*\"think\"\n",
      "Score: 0.012500000186264515\t Topic: 0.010*\"wire\" + 0.007*\"articl\" + 0.006*\"state\" + 0.005*\"time\" + 0.004*\"univers\"\n",
      "Score: 0.012500000186264515\t Topic: 0.008*\"file\" + 0.007*\"like\" + 0.006*\"entri\" + 0.006*\"articl\" + 0.005*\"mail\"\n",
      "Score: 0.012500000186264515\t Topic: 0.010*\"know\" + 0.009*\"peopl\" + 0.006*\"say\" + 0.005*\"think\" + 0.005*\"like\"\n",
      "Score: 0.012500000186264515\t Topic: 0.010*\"articl\" + 0.008*\"sandvik\" + 0.007*\"univers\" + 0.006*\"christian\" + 0.006*\"appl\"\n",
      "Score: 0.012500000186264515\t Topic: 0.007*\"articl\" + 0.007*\"like\" + 0.007*\"know\" + 0.006*\"univers\" + 0.005*\"say\"\n",
      "Score: 0.012500000186264515\t Topic: 0.011*\"nntp\" + 0.011*\"host\" + 0.010*\"articl\" + 0.007*\"think\" + 0.006*\"john\"\n",
      "Score: 0.012500000186264515\t Topic: 0.012*\"card\" + 0.008*\"articl\" + 0.008*\"know\" + 0.007*\"univers\" + 0.007*\"like\"\n",
      "Score: 0.012500000186264515\t Topic: 0.011*\"univers\" + 0.009*\"host\" + 0.009*\"nntp\" + 0.008*\"space\" + 0.007*\"nasa\"\n",
      "Score: 0.012500000186264515\t Topic: 0.017*\"drive\" + 0.014*\"scsi\" + 0.010*\"articl\" + 0.008*\"nntp\" + 0.008*\"host\"\n",
      "Score: 0.012500000186264515\t Topic: 0.008*\"israel\" + 0.008*\"articl\" + 0.007*\"know\" + 0.007*\"isra\" + 0.007*\"think\"\n",
      "Score: 0.012500000186264515\t Topic: 0.009*\"armenian\" + 0.008*\"file\" + 0.007*\"program\" + 0.007*\"imag\" + 0.006*\"avail\"\n",
      "Score: 0.012500000186264515\t Topic: 0.009*\"file\" + 0.008*\"like\" + 0.006*\"know\" + 0.006*\"articl\" + 0.005*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = ''\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Model Perplexity and Coherence Score\n",
    "Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is.  Topic coherence score, in particular, has been more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.792263501387259\n",
      "\n",
      "Coherence Score:  0.40946401358521706\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(bow_corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "    https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
